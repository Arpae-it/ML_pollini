cluster_id,model_name,exp_id,batch_size,dropout,layer_dense_units,layer_lstm_1_units,layer_lstm_2_units,learning_rate,sequence_len,test_error
0,LSTM_Attention_v1,2,1024,0.1,64,512,256,0.0023,7,0.052
1,LSTM_Attention_v1,8,1024,0.6,128,256,128,0.0068,21,0.0699
2,LSTM_Attention_v1,4,256,0.9,512,128,64,0.0485,7,0.1165
3,LSTM_Attention_v1,7,512,0.8,128,256,128,0.0214,21,0.1345
4,LSTM_Attention_v1,9,1024,0.1,64,128,64,0.0017,14,0.0337
5,LSTM_Attention_v1,1,1024,0.8,256,512,512,0.0011,21,0.0594
6,LSTM_Attention_v1,2,256,0.7,512,64,128,0.0634,21,0.0
7,LSTM_Attention_v1,1,1024,0.5,64,64,512,0.0487,21,0.0
8,LSTM_Attention_v1,2,256,0.8,64,128,64,0.0336,14,0.1287
9,LSTM_Attention_v1,8,256,0.8,256,512,64,0.0729,7,0.079
