{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27e9f867",
   "metadata": {},
   "source": [
    "# Experiment on Cluster TSD Season\n",
    "Notebook to perform some model on a single TSD Season Cluster: trhougth time-series-decomposition, the season inside the cluster is extracted and used to make prediction.<br>\n",
    "Data read are from table SLIDING_WINDOWS_DATASET that contains a sliding windows of:\n",
    "- feats about last 7-days meteo values\n",
    "- pollen value for the next day\n",
    "\n",
    "Cluster associations are read from a local file: we have different cluster annotations made by different techniques.<br>\n",
    "We explore different model & hyper-parameters throught Comet ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96a293a",
   "metadata": {},
   "source": [
    "<h3>Import</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80880ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.18.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "from datetime import timedelta\n",
    "from google.cloud import bigquery\n",
    "\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import get_cmap\n",
    "from matplotlib import cm\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "import ipywidgets as widgets\n",
    "import scipy.stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "\n",
    "from comet_ml import Experiment\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers.experimental import Adam, AdamW, Adadelta\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "my_cmap = plt.get_cmap(\"Paired\")\n",
    "init_notebook_mode(connected=True)  \n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7c4047",
   "metadata": {},
   "source": [
    "<h3>Config</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7031950c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "\n",
    "PROJECT_ID = 'arpae-prod-ml'\n",
    "\n",
    "# BigQuery\n",
    "BQ_DATASET = 'SAMPLE_DATA'\n",
    "JOINED_BQ_DATASET = 'JOINED_DATA'\n",
    "\n",
    "# Const\n",
    "COMMON_PERIOD_INIT = '2011-01-01'\n",
    "COMMON_PERIOD_END = '2023-12-31' \n",
    "\n",
    "TRAIN_END = '2016-12-31 00:00:00+00:00'\n",
    "VAL_END = '2019-12-31 00:00:00+00:00'\n",
    "TEST_END = '2022-12-31 00:00:00+00:00'\n",
    "\n",
    "# Cols\n",
    "DATE_COL = 'date'\n",
    "\n",
    "# Feats\n",
    "METEO_FEATS = ['week_amax', \n",
    "               'station_lat_amax', 'station_lon_amax', 'station_H_piano_strada_amax', 'station_H_mslm_amax', \n",
    "               'B13011_min_amin', 'B13011_max_amax', 'B13011_mean_mean', 'B13011_std_mean', 'B13011_sum_sum', \n",
    "               'B14198_min_amin', 'B14198_max_amax', 'B14198_mean_mean', 'B14198_std_mean', 'B14198_sum_sum',\n",
    "               'TEMP_min_amin', 'TEMP_max_amax', 'TEMP_mean_mean', 'TEMP_std_mean', 'TEMP_sum_sum',\n",
    "               'PREC_amin', 'PREC_mean', 'PREC_std', 'PREC_median', 'PREC_amax', 'PREC_skew', 'PREC_kurtosis']\n",
    "POLLEN_FEATS = ['seasonal_mean', 'seasonal_prev_1', # seasonal, trend, residual\n",
    "                'pol_value_amin', 'pol_value_mean', 'pol_value_std', 'pol_value_median', 'pol_value_amax', \n",
    "                'pol_value_skew', 'pol_value_kurtosis',\n",
    "                'pol_value_prev_1', 'pol_value_prev_2', 'pol_value_prev_3',\n",
    "                'pol_value_prev_4', 'pol_value_prev_5', 'pol_value_prev_6',\n",
    "                'pol_value_prev_7']\n",
    "ORIGINAL_FEATS = METEO_FEATS + POLLEN_FEATS\n",
    "LABEL_COL = 'season_label' # season, trend, residual\n",
    "\n",
    "# Comet Params\n",
    "COMET_API_KEY = 'B4Tttbbx4JrwXD9x2HBNjCdXX'\n",
    "COMET_WORKSPACE = 'pveronesi' \n",
    "COMET_PROJECT_NAME = 'arpae-tsd-seasonal-experiments' # season, trend, residual\n",
    "\n",
    "# Layout\n",
    "COLOR_PALETTE = px.colors.qualitative.Prism\n",
    "\n",
    "# Paths\n",
    "OUTPUT_CLUSTER_FILENAME = \"../../data/clustering_season_intervals.csv\" # season, trend, residual\n",
    "OPT_PARAMS_FILENAME = \"../../data/optimal_params_season.csv\" # season, trend, residual\n",
    "MODEL_DIR = \"../../models/season/\" # season, trend, residual\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b40c5c9",
   "metadata": {},
   "source": [
    "<h3>Methods</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ccb72ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Methods\n",
    "\n",
    "def _run_query(client, query): \n",
    "    df = client.query(query).to_dataframe()\n",
    "    return df\n",
    "\n",
    "def _read_table(client, project_id, dataset, table):\n",
    "    query = \"SELECT * FROM `{}.{}.{}` \".format(project_id, dataset, table)\n",
    "    df = _run_query(client, query)\n",
    "    return df\n",
    "\n",
    "def _read_table_delta(client, project_id, dataset, table, date_col, init, end):\n",
    "    query = \"SELECT * FROM `{}.{}.{}` WHERE {} > '{}' AND {} < '{}' \".format(project_id, dataset, table, date_col, init, date_col, end)\n",
    "    df = _run_query(client, query)\n",
    "    if 'reftime' in df.columns:\n",
    "        df.sort_values(by='reftime', inplace=True)\n",
    "    elif date_col in df.columns:\n",
    "        df.sort_values(by=date_col, inplace=True)\n",
    "    else:\n",
    "        return None\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8cf45c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comet methods\n",
    "\n",
    "def _create_experiment(api_key, workspace, project_name):\n",
    "    experiment = Experiment(\n",
    "        # user config\n",
    "        api_key=api_key,\n",
    "        workspace=workspace,  \n",
    "        # project config\n",
    "        project_name=project_name,\n",
    "        # logging config\n",
    "        log_code=True,\n",
    "        log_graph=True,\n",
    "        auto_param_logging=True,\n",
    "        auto_metric_logging=True,    \n",
    "        auto_histogram_weight_logging=True,\n",
    "        auto_histogram_gradient_logging=True,\n",
    "        auto_histogram_activation_logging=True\n",
    "    )\n",
    "    return experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36411aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Methods\n",
    "\n",
    "def _create_experiment_widget():\n",
    "    exp_wdgt = widgets.Dropdown(options=['Opt Params', 'Manual Params'], description='Set Params:', layout={\"width\":\"50%\"})\n",
    "    return exp_wdgt\n",
    "\n",
    "def _create_cluster_widget(clusters):\n",
    "    cluster_wdgt = widgets.Dropdown(options=clusters, description='Cluster id:', layout={\"width\":\"50%\"})\n",
    "    return cluster_wdgt\n",
    "\n",
    "def _normalize(x, range_dict, index_col, label_col):\n",
    "    pol_min = range_dict[x[index_col]]['min']\n",
    "    pol_max = range_dict[x[index_col]]['max']\n",
    "    return (x[label_col] - pol_min) / (pol_max - pol_min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37550651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Methods\n",
    "\n",
    "def _get_data(data_df, clusters_df, cluster_id, feats_cols, date_col, label_col):\n",
    "    # filter data\n",
    "    filt_clusters_df = clusters_df[clusters_df['cluster']==cluster_id][['station_id', 'pol_var_id']]\n",
    "    dataset_df = pd.merge(data_df, filt_clusters_df, how='right', on=['station_id', 'pol_var_id'])\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset_df.sort_values(['station_id', 'pol_var_id', date_col], inplace=True)\n",
    "    dataset_df = dataset_df[['station_id', 'pol_var_id', date_col]  + feats_cols + [label_col]]\n",
    "    \n",
    "    # Set index and drop nan\n",
    "    dataset_df.set_index(date_col, inplace=True)\n",
    "    dataset_df.dropna(inplace=True)\n",
    "    \n",
    "    print(\"Rows: {}\".format(dataset_df.shape[0]))\n",
    "    return dataset_df\n",
    "\n",
    "def _prepare_data(dataset_df, original_feats, meteo_feats, pollen_feats, label_col):\n",
    "    # Add 1-hot encoding cols\n",
    "    stations_one_hot = pd.get_dummies(dataset_df['station_id'], prefix='station_id')\n",
    "    pollen_one_hot = pd.get_dummies(dataset_df['pol_var_id'], prefix='pol_var_id')\n",
    "    dataset_df = pd.concat([dataset_df, stations_one_hot], axis=1)\n",
    "    dataset_df = pd.concat([dataset_df, pollen_one_hot], axis=1)\n",
    "    \n",
    "    # Update Features\n",
    "    feats = original_feats + stations_one_hot.columns.values.tolist() + pollen_one_hot.columns.values.tolist()\n",
    "    n_feats = len(feats)\n",
    "    \n",
    "    # Normalize all cols except for pollen ones; Save max cols to restore original values\n",
    "    cols_max = []\n",
    "    for col in meteo_feats:\n",
    "        scaler = MinMaxScaler()\n",
    "        dataset_df[col] = pd.DataFrame(scaler.fit_transform(dataset_df[[col]])).values\n",
    "        cols_max.append(int(scaler.data_max_))\n",
    "\n",
    "    # Normalize with Min-Max scaling each pollen feats \n",
    "    for col in tqdm(pollen_feats):\n",
    "        range_df = dataset_df[['pol_var_id', col]].groupby('pol_var_id').agg(['min', 'max'])\n",
    "        ranges = {}\n",
    "        for index, values in zip(range_df.index, range_df.values):\n",
    "            ranges[index] = {'min': values[0], 'max': values[1]}        \n",
    "        dataset_df[col] = dataset_df.apply(lambda x: _normalize(x, ranges, 'pol_var_id', col), axis=1)    \n",
    "    \n",
    "    # Normalize with Min-Max scaling the label col\n",
    "    range_df = dataset_df[['pol_var_id', label_col]].groupby('pol_var_id').agg(['min', 'max'])\n",
    "    ranges = {}\n",
    "    for index, values in zip(range_df.index, range_df.values):\n",
    "        ranges[index] = {'min': values[0], 'max': values[1]}        \n",
    "    dataset_df[label_col] = dataset_df.apply(lambda x: _normalize(x, ranges, 'pol_var_id', label_col), axis=1)    \n",
    "    \n",
    "    # Sort data\n",
    "    dataset_df.index = pd.to_datetime(dataset_df.index)\n",
    "    dataset_df.sort_values(by=['station_id', 'pol_var_id', 'date'], inplace=True)\n",
    "\n",
    "    return dataset_df, feats, n_feats, cols_max\n",
    "\n",
    "def _create_datasets(dataset_df, train_end, val_end, test_end, feats, label_col, sequence_len, batch_size):\n",
    "    # Split df into train and test sets\n",
    "    train_df = dataset_df[dataset_df.index < pd.to_datetime(train_end)]\n",
    "    val_df = dataset_df[(dataset_df.index > pd.to_datetime(train_end)) & \n",
    "                        (dataset_df.index < pd.to_datetime(val_end))]\n",
    "    test_df = dataset_df[(dataset_df.index > pd.to_datetime(val_end)) & \n",
    "                         (dataset_df.index < pd.to_datetime(test_end))]\n",
    "    print(\"Train dataset: {}, Val dataset: {}, Test dataset: {}\".format(train_df.shape[0], val_df.shape[0], test_df.shape[0]))\n",
    "    \n",
    "    # Split into feats and labels\n",
    "    train_X, train_y = train_df[feats].values, train_df[label_col]\n",
    "    val_X, val_y = val_df[feats].values, val_df[label_col]\n",
    "    test_X, test_y = test_df[feats].values, test_df[label_col]\n",
    "\n",
    "    # Create Sliding-Windows Dataset\n",
    "    train_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(train_X,\n",
    "                                                                         train_y,\n",
    "                                                                         sequence_length=sequence_len,\n",
    "                                                                         batch_size=batch_size)\n",
    "    val_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(val_X,\n",
    "                                                                       val_y,\n",
    "                                                                       sequence_length=sequence_len,\n",
    "                                                                       batch_size=batch_size)\n",
    "    test_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(test_X,\n",
    "                                                                        test_y,\n",
    "                                                                        sequence_length=sequence_len,\n",
    "                                                                        batch_size=batch_size)\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bbf78a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Methods\n",
    "\n",
    "def _lstm_base_model(sequence_len, n_feats, dropout, layer_lstm_1_units, layer_lstm_2_units, \n",
    "                     layer_dense_units):\n",
    "    input_layer = tf.keras.layers.Input(shape=(sequence_len, n_feats))\n",
    "    x = tf.keras.layers.LSTM(units=layer_lstm_1_units, \n",
    "                                  dropout=dropout, \n",
    "                                  return_sequences=True)(input_layer)\n",
    "    x = tf.keras.layers.LSTM(units=layer_lstm_2_units, \n",
    "                                  dropout=dropout, \n",
    "                                  return_sequences=True)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "    x = tf.keras.layers.Dense(units=layer_dense_units)(x)\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "    output_layer = tf.keras.layers.Dense(units=1)(x)\n",
    "    model = tf.keras.models.Model(inputs=input_layer, outputs=output_layer) \n",
    "    return model\n",
    "\n",
    "def _lstm_attention_model(sequence_len, n_feats):\n",
    "    input_layer = tf.keras.layers.Input(shape=(sequence_len, n_feats))\n",
    "    x = tf.keras.layers.LSTM(units=128, \n",
    "                                  dropout=0.4, \n",
    "                                  return_sequences=True)(input_layer)\n",
    "    x = tf.keras.layers.LSTM(units=128, \n",
    "                                  dropout=0.4, \n",
    "                                  return_sequences=True)(x)\n",
    "    x = tf.keras.layers.MultiHeadAttention(num_heads=8, key_dim=64)(query=x, value=x, key=x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dropout(0.4)(x)\n",
    "    x = tf.keras.layers.Dense(units=128)(x)\n",
    "    x = tf.keras.layers.Dropout(0.4)(x)\n",
    "    output_layer = tf.keras.layers.Dense(units=1)(x)\n",
    "    model = tf.keras.models.Model(inputs=input_layer, outputs=output_layer) \n",
    "    return model\n",
    "\n",
    "# Losses Methods\n",
    "\n",
    "def _compile_mse_model(model):\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min', restore_best_weights=True)\n",
    "    model.summary()\n",
    "    return model, early_stop\n",
    "\n",
    "def _compile_adamw_mse_model(model, learning_rate, weight_decay):\n",
    "    # AdamW \n",
    "    optimizer = AdamW(learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min', restore_best_weights=True)\n",
    "    model.summary()\n",
    "    return model, early_stop\n",
    "\n",
    "def _compile_adadelta_mse_model(model, learning_rate, rho):\n",
    "    # Adadelta is based on adaptive learning rate\n",
    "    optimizer = Adadelta(learning_rate=learning_rate, rho=rho)\n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min', restore_best_weights=True)\n",
    "    model.summary()\n",
    "    return model, early_stop\n",
    "\n",
    "# Training Methods\n",
    "\n",
    "def _fit_model(model, epochs, train_dataset, val_dataset, callbacks):    \n",
    "    history = model.fit(train_dataset, \n",
    "                        epochs=epochs,\n",
    "                        validation_data=val_dataset, \n",
    "                        verbose=1, \n",
    "                        callbacks=callbacks,\n",
    "                        shuffle=True)\n",
    "    return history\n",
    "\n",
    "def _run_experiment(comet_api_key, comet_workspace, comet_project, data_df, clusters_df,\n",
    "                    cluster_id, tag,\n",
    "                    model_name, batch_size, sequence_len, loss, learning_rate, weight_decay, rho, epochs,\n",
    "                    dropout, layer_lstm_1_units, layer_lstm_2_units, layer_dense_units,\n",
    "                    original_feats, meteo_feats, pollen_feats, date_col, label_col,\n",
    "                    train_end, val_end, test_end):\n",
    "    # Run Experiment\n",
    "    model_id = \"{}-Batch{}-SeqLen{}-Loss{}-Cluster{}\".format(model_name, batch_size, sequence_len, \n",
    "                                                             loss, cluster_id)\n",
    "    experiment = _create_experiment(comet_api_key, comet_workspace, comet_project)\n",
    "    experiment.set_name(model_id)\n",
    "    experiment.add_tag(tag)\n",
    "    print(\"Running Experiment {}:\".format(model_id))\n",
    "\n",
    "    # Get Data\n",
    "    print(\"\\nGetting data..\")\n",
    "    dataset_df = _get_data(data_df, clusters_df, cluster_id, original_feats, date_col, label_col)    \n",
    "\n",
    "    # Prepare Data\n",
    "    print(\"\\nPreparing data..\")\n",
    "    dataset_df, feats, n_feats, cols_max = _prepare_data(dataset_df, original_feats, meteo_feats, \n",
    "                                                         pollen_feats, label_col)\n",
    "\n",
    "    # Create Dataset\n",
    "    print(\"\\nCreating dataset..\")\n",
    "    train_dataset, val_dataset, test_dataset, test_df = _create_datasets(dataset_df, train_end, val_end, \n",
    "                                                                         test_end, feats, label_col, \n",
    "                                                                         sequence_len, batch_size)\n",
    "\n",
    "    # Define Model\n",
    "    print(\"\\nDefining & Training model..\")\n",
    "    if model_name == 'LSTM_base':\n",
    "        model = _lstm_base_model(sequence_len, n_feats, dropout, layer_lstm_1_units, layer_lstm_2_units, \n",
    "                                 layer_dense_units)\n",
    "    elif model_name == 'LSTM_Attention_v1':\n",
    "        model = _lstm_attention_model(sequence_len, n_feats)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # Compile Model\n",
    "    if loss == 'ADAM_MSE':\n",
    "        model, early_stop = _compile_mse_model(model)\n",
    "    elif loss == 'ADAMW_MSE':\n",
    "        model, early_stop = _compile_adamw_mse_model(model, learning_rate, weight_decay)\n",
    "    elif loss == 'ADADELTA_MSE':\n",
    "        model, early_stop = _compile_adadelta_mse_model(model, learning_rate, rho)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # Train Model\n",
    "    history = _fit_model(model, epochs, train_dataset, val_dataset, [early_stop])\n",
    "\n",
    "    # Get Error on test-set\n",
    "    preds, error = _get_error(model, test_dataset)\n",
    "    experiment.log_other(\"test-error\", error)\n",
    "    print(\"Final Error: {}\".format(error))\n",
    "\n",
    "    # Save Model (locally and on comet)\n",
    "    model_path = os.path.join(MODEL_DIR, model_id)\n",
    "    print(\"Saving Model on {} ..\".format(model_path))\n",
    "    model.save(model_path)\n",
    "    experiment.log_model(model_id, model_path)\n",
    "    \n",
    "    # End experiment\n",
    "    experiment.end()\n",
    "    \n",
    "    return history, test_dataset, test_df, preds, error, feats\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52f501a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Methods\n",
    "\n",
    "def _get_error(model, test_dataset):\n",
    "    preds = model.predict(test_dataset).squeeze()\n",
    "    truth = []\n",
    "    for x, y in test_dataset:\n",
    "        truth.extend(y.numpy())\n",
    "    error = np.mean(np.abs(preds-np.array(truth)))\n",
    "    return preds, error\n",
    "\n",
    "def _feats_importances(test_dataset, feats, sequence_len):\n",
    "    # Get baseline error\n",
    "    feats_imp = []\n",
    "    ff_preds = model.predict(test_dataset, verbose=0).squeeze()\n",
    "    ff_x, ff_y = [], []\n",
    "    for x, y in test_dataset:\n",
    "        ff_x.extend(x.numpy())\n",
    "        ff_y.extend(y.numpy())\n",
    "    ff_x, ff_y = np.array(ff_x), np.array(ff_y)\n",
    "    baseline_error = np.mean(np.abs(ff_preds-np.array(ff_y)))\n",
    "    feats_imp.append({'feature':'BASELINE','mae': baseline_error})\n",
    "    \n",
    "    # Get features gain on reducing error: each value \n",
    "    for k in tqdm(range(len(feats))):\n",
    "        # Change values for current feat\n",
    "        save_col = ff_x[:,:,k].copy()\n",
    "        ff_x[:,:,k] = [-100 for x in range(sequence_len)]\n",
    "        # Compute error \n",
    "        oof_preds = model.predict(ff_x, verbose=0).squeeze() \n",
    "        mae = np.mean(np.abs(oof_preds-ff_y))\n",
    "        feats_imp.append({'feature': feats[k],'mae': mae})\n",
    "        ff_x[:,:,k] = save_col\n",
    "    \n",
    "    # Plot \n",
    "    df = pd.DataFrame(feats_imp)\n",
    "    df = df.sort_values('mae')\n",
    "    plt.figure(figsize=(10, 13))\n",
    "    plt.barh(np.arange(len(feats)+1), df.mae)\n",
    "    plt.yticks(np.arange(len(feats)+1), df.feature.values)\n",
    "    plt.title('LSTM Feature Importance', size=16)\n",
    "    plt.ylim((-1, len(feats)+1))\n",
    "    plt.plot([baseline_error, baseline_error], [-1,len(feats)+1], '--', color='orange',\n",
    "             label=f'Baseline OOF\\nMAE={baseline_error:.3f}')\n",
    "    plt.xlabel('MAE with feature permuted', size=14)\n",
    "    plt.ylabel('Feature', size=14)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def _plot_history(history):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def _plot_preds(preds, test_df, LABEL_COL):\n",
    "    # Predict & Assign to test_df: add values for missing days in preds\n",
    "    for i in range(len(preds), test_df.shape[0]):\n",
    "        preds = np.append(preds, 0.0)\n",
    "    test_df['preds'] = preds\n",
    "    # Plot some station & bcode\n",
    "    N_SAMPLE = min(10, test_df[['station_id', 'pol_var_id']].drop_duplicates().shape[0])\n",
    "    for station_id, pol_var_id in test_df[['station_id', 'pol_var_id']].drop_duplicates().sample(N_SAMPLE).values:\n",
    "        curr_test_df = test_df[(test_df['station_id']==station_id) & (test_df['pol_var_id']==pol_var_id)]\n",
    "        curr_test_df.sort_index(inplace=True)\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.title(\"{} - {}\".format(station_id, pol_var_id))\n",
    "        plt.plot(curr_test_df.index, curr_test_df['preds'], label='pred')\n",
    "        plt.plot(curr_test_df.index, curr_test_df[LABEL_COL], label='truth')\n",
    "        plt.ylim(0, 1)\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeeb4fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3b476b0",
   "metadata": {},
   "source": [
    "<h3>1. Config</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8487293b",
   "metadata": {},
   "source": [
    "<h4>1.1 Config BigQuery</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d526cc1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.client.Client at 0x15be4fa90>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup Client\n",
    "\n",
    "bq_client = bigquery.Client(project=PROJECT_ID)\n",
    "bq_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5ae261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e6b9aa3",
   "metadata": {},
   "source": [
    "<h3>2. Read Data</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185c05b0",
   "metadata": {},
   "source": [
    "<h4>2.1 Read Cluster file</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2fac3602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(363, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>pol_var_id</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B48001</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>B48002</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>B48003</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   station_id pol_var_id  cluster\n",
       "0           1     B48001        5\n",
       "1           1     B48002        4\n",
       "2           1     B48003        4"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters_df = pd.read_csv(OUTPUT_CLUSTER_FILENAME)\n",
    "print(clusters_df.shape)\n",
    "clusters_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83089647",
   "metadata": {},
   "source": [
    "<h4>2.2 Read Optimal Params (if setted)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc0503fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>model_name</th>\n",
       "      <th>exp_id</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>dropout</th>\n",
       "      <th>layer_dense_units</th>\n",
       "      <th>layer_lstm_1_units</th>\n",
       "      <th>layer_lstm_2_units</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>sequence_len</th>\n",
       "      <th>test_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>LSTM_base</td>\n",
       "      <td>7</td>\n",
       "      <td>512</td>\n",
       "      <td>0.9</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0576</td>\n",
       "      <td>14</td>\n",
       "      <td>0.0441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>LSTM_base</td>\n",
       "      <td>10</td>\n",
       "      <td>512</td>\n",
       "      <td>0.7</td>\n",
       "      <td>128</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0313</td>\n",
       "      <td>21</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>LSTM_base</td>\n",
       "      <td>6</td>\n",
       "      <td>512</td>\n",
       "      <td>0.3</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0804</td>\n",
       "      <td>14</td>\n",
       "      <td>0.0522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>LSTM_base</td>\n",
       "      <td>5</td>\n",
       "      <td>512</td>\n",
       "      <td>0.1</td>\n",
       "      <td>512</td>\n",
       "      <td>64</td>\n",
       "      <td>512</td>\n",
       "      <td>0.0651</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>LSTM_base</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>0.1</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0716</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>LSTM_base</td>\n",
       "      <td>8</td>\n",
       "      <td>512</td>\n",
       "      <td>0.9</td>\n",
       "      <td>128</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>14</td>\n",
       "      <td>0.0570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>LSTM_base</td>\n",
       "      <td>9</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>LSTM_base</td>\n",
       "      <td>7</td>\n",
       "      <td>256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>21</td>\n",
       "      <td>0.0488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>LSTM_base</td>\n",
       "      <td>5</td>\n",
       "      <td>512</td>\n",
       "      <td>0.9</td>\n",
       "      <td>128</td>\n",
       "      <td>256</td>\n",
       "      <td>512</td>\n",
       "      <td>0.0685</td>\n",
       "      <td>21</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>LSTM_base</td>\n",
       "      <td>5</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.3</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0109</td>\n",
       "      <td>14</td>\n",
       "      <td>0.0510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster_id model_name  exp_id  batch_size  dropout  layer_dense_units  \\\n",
       "0           0  LSTM_base       7         512      0.9                128   \n",
       "1           1  LSTM_base      10         512      0.7                128   \n",
       "2           2  LSTM_base       6         512      0.3                128   \n",
       "3           3  LSTM_base       5         512      0.1                512   \n",
       "4           4  LSTM_base       5         256      0.1                 64   \n",
       "5           5  LSTM_base       8         512      0.9                128   \n",
       "6           6  LSTM_base       9        1024      0.5                 64   \n",
       "7           7  LSTM_base       7         256      0.5                512   \n",
       "8           8  LSTM_base       5         512      0.9                128   \n",
       "9           9  LSTM_base       5        1024      0.3                128   \n",
       "\n",
       "   layer_lstm_1_units  layer_lstm_2_units  learning_rate  sequence_len  \\\n",
       "0                  64                 256         0.0576            14   \n",
       "1                 256                 256         0.0313            21   \n",
       "2                 128                 128         0.0804            14   \n",
       "3                  64                 512         0.0651             7   \n",
       "4                  64                 128         0.0716             7   \n",
       "5                 256                 128         0.0022            14   \n",
       "6                 128                  64         0.0076             7   \n",
       "7                 256                 256         0.0061            21   \n",
       "8                 256                 512         0.0685            21   \n",
       "9                  64                  64         0.0109            14   \n",
       "\n",
       "   test_error  \n",
       "0      0.0441  \n",
       "1      0.0000  \n",
       "2      0.0522  \n",
       "3      0.0288  \n",
       "4      0.0314  \n",
       "5      0.0570  \n",
       "6      0.0321  \n",
       "7      0.0488  \n",
       "8      0.0000  \n",
       "9      0.0510  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists(OPT_PARAMS_FILENAME):\n",
    "    print(\"Params File Not Found.\")    \n",
    "\n",
    "params_df = pd.read_csv(OPT_PARAMS_FILENAME)\n",
    "print(params_df.shape)\n",
    "params_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b74ed53",
   "metadata": {},
   "source": [
    "<h4>2.3 Read Tables</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d44bb4",
   "metadata": {},
   "source": [
    "<b>SLIDING_WINDOWS_DATASET</b> joins meteo features of last 7-days with the next-day pollen value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6e59922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(826748, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>station_id</th>\n",
       "      <th>pol_var_id</th>\n",
       "      <th>date_diff</th>\n",
       "      <th>B13011_min_amin</th>\n",
       "      <th>B13011_max_amax</th>\n",
       "      <th>B13011_mean_mean</th>\n",
       "      <th>B13011_std_mean</th>\n",
       "      <th>B13011_sum_sum</th>\n",
       "      <th>B14198_min_amin</th>\n",
       "      <th>...</th>\n",
       "      <th>pol_var_id_B48033</th>\n",
       "      <th>pol_var_id_B48034</th>\n",
       "      <th>pol_var_id_B48036</th>\n",
       "      <th>pol_var_id_B48037</th>\n",
       "      <th>pol_var_id_B48038</th>\n",
       "      <th>pol_var_id_B48039</th>\n",
       "      <th>pol_var_id_B48041</th>\n",
       "      <th>pol_var_id_B48044</th>\n",
       "      <th>pol_var_id_B48045</th>\n",
       "      <th>WHICH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>325156</th>\n",
       "      <td>2011-01-02 00:00:00+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>B48008</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.8</td>\n",
       "      <td>0.011261</td>\n",
       "      <td>0.108532</td>\n",
       "      <td>13.4</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314359</th>\n",
       "      <td>2011-01-02 00:00:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>B48034</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.6</td>\n",
       "      <td>0.014790</td>\n",
       "      <td>0.148225</td>\n",
       "      <td>17.6</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590215</th>\n",
       "      <td>2011-01-02 00:00:00+00:00</td>\n",
       "      <td>13</td>\n",
       "      <td>B48003</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.004450</td>\n",
       "      <td>0.039220</td>\n",
       "      <td>3.8</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             date  station_id pol_var_id  date_diff  \\\n",
       "325156  2011-01-02 00:00:00+00:00           2     B48008         15   \n",
       "314359  2011-01-02 00:00:00+00:00           1     B48034          7   \n",
       "590215  2011-01-02 00:00:00+00:00          13     B48003         10   \n",
       "\n",
       "        B13011_min_amin  B13011_max_amax  B13011_mean_mean  B13011_std_mean  \\\n",
       "325156              0.0              8.8          0.011261         0.108532   \n",
       "314359              0.0             12.6          0.014790         0.148225   \n",
       "590215              0.0              2.4          0.004450         0.039220   \n",
       "\n",
       "        B13011_sum_sum  B14198_min_amin  ...  pol_var_id_B48033  \\\n",
       "325156            13.4            -10.0  ...                  0   \n",
       "314359            17.6             -9.0  ...                  0   \n",
       "590215             3.8             -8.0  ...                  0   \n",
       "\n",
       "        pol_var_id_B48034  pol_var_id_B48036  pol_var_id_B48037  \\\n",
       "325156                  0                  0                  0   \n",
       "314359                  1                  0                  0   \n",
       "590215                  0                  0                  0   \n",
       "\n",
       "        pol_var_id_B48038  pol_var_id_B48039  pol_var_id_B48041  \\\n",
       "325156                  0                  0                  0   \n",
       "314359                  0                  0                  0   \n",
       "590215                  0                  0                  0   \n",
       "\n",
       "        pol_var_id_B48044  pol_var_id_B48045     WHICH  \n",
       "325156                  0                  0  training  \n",
       "314359                  0                  0  training  \n",
       "590215                  0                  0  training  \n",
       "\n",
       "[3 rows x 100 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read SLIDING_WINDOWS_DATASET\n",
    "\n",
    "sliding_windows_dataset_df = _read_table_delta(bq_client, PROJECT_ID, JOINED_BQ_DATASET, \n",
    "                                               \"SLIDING_WINDOWS_DATASET\", \"date\",\n",
    "                                               COMMON_PERIOD_INIT, COMMON_PERIOD_END)\n",
    "sliding_windows_dataset_df['date'] = sliding_windows_dataset_df['date'].astype(\"str\")\n",
    "print(sliding_windows_dataset_df.shape)\n",
    "sliding_windows_dataset_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c87666c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6270912c",
   "metadata": {},
   "source": [
    "<h3>3. Run Experiment</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba676c3",
   "metadata": {},
   "source": [
    "<h4>3.1 Config Experiment</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0ac024c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data\n",
    "\n",
    "data_df = sliding_windows_dataset_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6c9d447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 clusters in data\n"
     ]
    }
   ],
   "source": [
    "# Get Clusters\n",
    "\n",
    "clusters = [3, 4, 5, 6, 7, 8, 9] #sorted(clusters_df.cluster.unique())\n",
    "print(\"Found {} clusters in data\".format(len(clusters)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dda0c4b",
   "metadata": {},
   "source": [
    "<h4>3.2 Set Params</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c28cfb",
   "metadata": {},
   "source": [
    "Set:\n",
    "- <b>Manual Params</b>: to use the params setted in the code\n",
    "- <b>Opt Params</b>: to read the optimal params from the output file after hyper-params tuning has run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0461e0e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cff2b2f5ff748c6a204db848f53bfda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Set Params:', layout=Layout(width='50%'), options=('Opt Params', 'Manual Params'), value…"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set Params\n",
    "\n",
    "exp_wdgt_value = _create_experiment_widget()\n",
    "exp_wdgt_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b1831c",
   "metadata": {},
   "source": [
    "<h4>3.2 Run Experiment</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "06326b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default Params\n",
    "\n",
    "DEFAULT_CLUSTER_ID = 0\n",
    "DEFAULT_BATCH_SIZE = 512               \n",
    "DEFAULT_SEQUENCE_LEN = 7\n",
    "DEFAULT_LEARNING_RATE = 0.001\n",
    "DEFAULT_WEIGHT_DECAY = 0.004\n",
    "DEFAULT_RHO = 0.95\n",
    "DEFAULT_EPOCHS = 50\n",
    "DEFAULT_MODEL_NAME = 'LSTM_base'  # LSTM_base, LSTM_Attention_v1\n",
    "DEFAULT_LOSS = 'ADAM_MSE'         # ADAM_MSE, ADAMW_MSE, ADADELTA_MSE\n",
    "\n",
    "DEFAULT_DROPOUT = 0.4\n",
    "DEFAULT_LAYER_LSTM_1_UNITS = 128\n",
    "DEFAULT_LAYER_LSTM_2_UNITS = 128\n",
    "DEFAULT_LAYER_DENSE_UNITS = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ddf5f620",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: sklearn. Metrics and hyperparameters can still be logged using Experiment.log_metrics() and Experiment.log_parameters()\n",
      "COMET INFO: ---------------------------------------------------------------------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------------------------------------------------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.com/pveronesi/arpae-tsd-seasonal-experiments/d1fcfe290590424184083ad2e2ce11f0\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     batch_loss [11] : (0.018365059047937393, 0.38030508160591125)\n",
      "COMET INFO:   Others:\n",
      "COMET INFO:     Name             : LSTM_base-Batch512-SeqLen7-LossADAM_MSE-Cluster3\n",
      "COMET INFO:     trainable_params : 3050753\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     Adam_amsgrad                 : False\n",
      "COMET INFO:     Adam_beta_1                  : 0.9\n",
      "COMET INFO:     Adam_beta_2                  : 0.999\n",
      "COMET INFO:     Adam_clipnorm                : 1\n",
      "COMET INFO:     Adam_clipvalue               : 1\n",
      "COMET INFO:     Adam_ema_momentum            : 0.99\n",
      "COMET INFO:     Adam_ema_overwrite_frequency : 1\n",
      "COMET INFO:     Adam_epsilon                 : 1e-07\n",
      "COMET INFO:     Adam_global_clipnorm         : 1\n",
      "COMET INFO:     Adam_is_legacy_optimizer     : False\n",
      "COMET INFO:     Adam_jit_compile             : False\n",
      "COMET INFO:     Adam_learning_rate           : 0.0010000000474974513\n",
      "COMET INFO:     Adam_name                    : Adam\n",
      "COMET INFO:     Adam_use_ema                 : False\n",
      "COMET INFO:     Adam_weight_decay            : 1\n",
      "COMET INFO:     epochs                       : 50\n",
      "COMET INFO:     steps                        : 199\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     environment details      : 1\n",
      "COMET INFO:     filename                 : 1\n",
      "COMET INFO:     git metadata             : 1\n",
      "COMET INFO:     git-patch (uncompressed) : 1 (265.25 KB)\n",
      "COMET INFO:     histogram3d              : 10\n",
      "COMET INFO:     installed packages       : 1\n",
      "COMET INFO:     model graph              : 1\n",
      "COMET INFO:     notebook                 : 1\n",
      "COMET INFO:     source_code              : 1\n",
      "COMET INFO: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Cluster_id: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: sklearn. Metrics and hyperparameters can still be logged using Experiment.log_metrics() and Experiment.log_parameters()\n",
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.com https://www.comet.com/pveronesi/arpae-tsd-seasonal-experiments/39e9a122c584492cbc17df2e4093cc90\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Experiment LSTM_base-Batch512-SeqLen7-LossADAM_MSE-Cluster3:\n",
      "\n",
      "Getting data..\n",
      "Rows: 171118\n",
      "\n",
      "Preparing data..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "929757f39b6f446fbfc497ef0e39f5fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating dataset..\n",
      "Train dataset: 101719, Val dataset: 46543, Test dataset: 22843\n",
      "\n",
      "Defining & Training model..\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 7, 64)]           0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 7, 64)             33024     \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 7, 512)            1181696   \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 3584)              0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 3584)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               1835520   \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,050,753\n",
      "Trainable params: 3,050,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: tensorflow datasets are not currently supported for gradient and activation auto-logging\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "199/199 [==============================] - 97s 468ms/step - loss: 0.0702 - val_loss: 0.0071\n",
      "Epoch 2/50\n",
      "199/199 [==============================] - 106s 532ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 3/50\n",
      "199/199 [==============================] - 117s 587ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 4/50\n",
      "199/199 [==============================] - 115s 579ms/step - loss: 0.0026 - val_loss: 0.0036\n",
      "Epoch 5/50\n",
      "199/199 [==============================] - 115s 576ms/step - loss: 0.0029 - val_loss: 0.0025\n",
      "Epoch 6/50\n",
      "199/199 [==============================] - 115s 578ms/step - loss: 0.0024 - val_loss: 0.0031\n",
      "Epoch 7/50\n",
      "199/199 [==============================] - 134s 674ms/step - loss: 0.0023 - val_loss: 0.0022\n",
      "Epoch 8/50\n",
      "199/199 [==============================] - 129s 651ms/step - loss: 0.0022 - val_loss: 0.0034\n",
      "Epoch 9/50\n",
      "199/199 [==============================] - 113s 568ms/step - loss: 0.0025 - val_loss: 0.0029\n",
      "Epoch 10/50\n",
      "199/199 [==============================] - 120s 604ms/step - loss: 0.0024 - val_loss: 0.0026\n",
      "Epoch 11/50\n",
      "199/199 [==============================] - 118s 592ms/step - loss: 0.0019 - val_loss: 0.0023\n",
      "Epoch 12/50\n",
      "199/199 [==============================] - 113s 569ms/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 13/50\n",
      "199/199 [==============================] - 130s 652ms/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 14/50\n",
      "199/199 [==============================] - 138s 696ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 15/50\n",
      "199/199 [==============================] - 142s 717ms/step - loss: 0.0015 - val_loss: 0.0013\n",
      "Epoch 16/50\n",
      "199/199 [==============================] - 139s 698ms/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 17/50\n",
      "199/199 [==============================] - 129s 649ms/step - loss: 0.0015 - val_loss: 0.0029\n",
      "Epoch 18/50\n",
      "199/199 [==============================] - 140s 706ms/step - loss: 0.0018 - val_loss: 0.0014\n",
      "Epoch 19/50\n",
      "199/199 [==============================] - 128s 645ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 20/50\n",
      "199/199 [==============================] - 136s 682ms/step - loss: 0.0012 - val_loss: 9.2269e-04\n",
      "Epoch 21/50\n",
      "199/199 [==============================] - 122s 613ms/step - loss: 0.0011 - val_loss: 8.8048e-04\n",
      "Epoch 22/50\n",
      "199/199 [==============================] - 115s 579ms/step - loss: 0.0010 - val_loss: 6.8576e-04\n",
      "Epoch 23/50\n",
      "199/199 [==============================] - 113s 567ms/step - loss: 8.7839e-04 - val_loss: 6.0714e-04\n",
      "Epoch 24/50\n",
      "199/199 [==============================] - 113s 568ms/step - loss: 8.4859e-04 - val_loss: 6.1721e-04\n",
      "Epoch 25/50\n",
      "199/199 [==============================] - 113s 567ms/step - loss: 8.0613e-04 - val_loss: 4.7484e-04\n",
      "Epoch 26/50\n",
      "199/199 [==============================] - 109s 547ms/step - loss: 8.4711e-04 - val_loss: 0.0013\n",
      "Epoch 27/50\n",
      "199/199 [==============================] - 110s 556ms/step - loss: 8.0472e-04 - val_loss: 6.4944e-04\n",
      "Epoch 28/50\n",
      "199/199 [==============================] - 111s 557ms/step - loss: 7.5561e-04 - val_loss: 5.9266e-04\n",
      "Epoch 29/50\n",
      "199/199 [==============================] - 111s 558ms/step - loss: 8.6650e-04 - val_loss: 7.3590e-04\n",
      "Epoch 30/50\n",
      "199/199 [==============================] - 110s 552ms/step - loss: 7.7874e-04 - val_loss: 5.9689e-04\n",
      "Epoch 31/50\n",
      "199/199 [==============================] - 110s 553ms/step - loss: 7.2927e-04 - val_loss: 0.0010\n",
      "Epoch 32/50\n",
      "199/199 [==============================] - 110s 555ms/step - loss: 7.4212e-04 - val_loss: 8.3310e-04\n",
      "Epoch 33/50\n",
      "199/199 [==============================] - 110s 552ms/step - loss: 9.4632e-04 - val_loss: 0.0024\n",
      "Epoch 34/50\n",
      "199/199 [==============================] - 110s 552ms/step - loss: 0.0010 - val_loss: 3.6433e-04\n",
      "Epoch 35/50\n",
      "199/199 [==============================] - 111s 557ms/step - loss: 7.3841e-04 - val_loss: 6.3633e-04\n",
      "Epoch 36/50\n",
      "199/199 [==============================] - 110s 553ms/step - loss: 7.4292e-04 - val_loss: 7.0009e-04\n",
      "Epoch 37/50\n",
      "199/199 [==============================] - 108s 545ms/step - loss: 7.4158e-04 - val_loss: 4.7874e-04\n",
      "Epoch 38/50\n",
      "199/199 [==============================] - 111s 558ms/step - loss: 7.4111e-04 - val_loss: 7.5328e-04\n",
      "Epoch 39/50\n",
      "199/199 [==============================] - 111s 561ms/step - loss: 6.2935e-04 - val_loss: 8.1534e-04\n",
      "Epoch 40/50\n",
      "199/199 [==============================] - 112s 564ms/step - loss: 7.4160e-04 - val_loss: 8.3733e-04\n",
      "Epoch 41/50\n",
      "199/199 [==============================] - 109s 551ms/step - loss: 8.0482e-04 - val_loss: 0.0012\n",
      "Epoch 42/50\n",
      "199/199 [==============================] - 110s 554ms/step - loss: 8.2004e-04 - val_loss: 5.0497e-04\n",
      "Epoch 43/50\n",
      "199/199 [==============================] - 108s 545ms/step - loss: 6.5872e-04 - val_loss: 8.5558e-04\n",
      "Epoch 44/50\n",
      "199/199 [==============================] - ETA: 0s - loss: 7.2029e-04Restoring model weights from the end of the best epoch: 34.\n",
      "199/199 [==============================] - 109s 549ms/step - loss: 7.2029e-04 - val_loss: 7.8714e-04\n",
      "Epoch 44: early stopping\n",
      "45/45 [==============================] - 9s 169ms/step\n",
      "Final Error: 0.013022032861696715\n",
      "Saving Model on ../../models/season/LSTM_base-Batch512-SeqLen7-LossADAM_MSE-Cluster3 ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/season/LSTM_base-Batch512-SeqLen7-LossADAM_MSE-Cluster3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/season/LSTM_base-Batch512-SeqLen7-LossADAM_MSE-Cluster3/assets\n",
      "COMET INFO: ---------------------------------------------------------------------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------------------------------------------------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.com/pveronesi/arpae-tsd-seasonal-experiments/39e9a122c584492cbc17df2e4093cc90\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     batch_loss [880]          : (0.00032960157841444016, 0.3370179235935211)\n",
      "COMET INFO:     epoch_duration [44]       : (97.00580581399993, 141.769057128)\n",
      "COMET INFO:     loss [44]                 : (0.0006293518817983568, 0.0701746866106987)\n",
      "COMET INFO:     val_loss [44]             : (0.0003643313830252737, 0.007087272591888905)\n",
      "COMET INFO:     validate_batch_loss [440] : (0.00010476851457497105, 0.007164034526795149)\n",
      "COMET INFO:   Others:\n",
      "COMET INFO:     Name             : LSTM_base-Batch512-SeqLen7-LossADAM_MSE-Cluster3\n",
      "COMET INFO:     test-error       : 0.013022032861696715\n",
      "COMET INFO:     trainable_params : 3050753\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     Adam_amsgrad                 : False\n",
      "COMET INFO:     Adam_beta_1                  : 0.9\n",
      "COMET INFO:     Adam_beta_2                  : 0.999\n",
      "COMET INFO:     Adam_clipnorm                : 1\n",
      "COMET INFO:     Adam_clipvalue               : 1\n",
      "COMET INFO:     Adam_ema_momentum            : 0.99\n",
      "COMET INFO:     Adam_ema_overwrite_frequency : 1\n",
      "COMET INFO:     Adam_epsilon                 : 1e-07\n",
      "COMET INFO:     Adam_global_clipnorm         : 1\n",
      "COMET INFO:     Adam_is_legacy_optimizer     : False\n",
      "COMET INFO:     Adam_jit_compile             : False\n",
      "COMET INFO:     Adam_learning_rate           : 0.0010000000474974513\n",
      "COMET INFO:     Adam_name                    : Adam\n",
      "COMET INFO:     Adam_use_ema                 : False\n",
      "COMET INFO:     Adam_weight_decay            : 1\n",
      "COMET INFO:     epochs                       : 50\n",
      "COMET INFO:     steps                        : 199\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     environment details      : 1\n",
      "COMET INFO:     filename                 : 1\n",
      "COMET INFO:     git metadata             : 1\n",
      "COMET INFO:     git-patch (uncompressed) : 1 (266.54 KB)\n",
      "COMET INFO:     histogram3d              : 450\n",
      "COMET INFO:     installed packages       : 1\n",
      "COMET INFO:     model graph              : 1\n",
      "COMET INFO:     model-element            : 5 (36.41 MB)\n",
      "COMET INFO:     notebook                 : 1\n",
      "COMET INFO:     source_code              : 1\n",
      "COMET INFO: \n",
      "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: sklearn. Metrics and hyperparameters can still be logged using Experiment.log_metrics() and Experiment.log_parameters()\n",
      "COMET INFO: Uploading metrics, params, and assets to Comet before program termination (may take several seconds)\n",
      "COMET INFO: The Python SDK has 3600 seconds to finish uploading collected data\n",
      "COMET INFO: Uploading 1 metrics, params and output messages\n",
      "COMET INFO: Waiting for completion of the file uploads (may take several seconds)\n",
      "COMET INFO: The Python SDK has 10800 seconds to finish uploading collected data\n",
      "COMET INFO: Still uploading 1 file(s), remaining 29.02 MB/37.57 MB\n",
      "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: sklearn. Metrics and hyperparameters can still be logged using Experiment.log_metrics() and Experiment.log_parameters()\n",
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Running Cluster_id: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Experiment is live on comet.com https://www.comet.com/pveronesi/arpae-tsd-seasonal-experiments/64bee0bab46b44b99e41ae40557ed850\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Experiment LSTM_base-Batch256-SeqLen7-LossADAM_MSE-Cluster4:\n",
      "\n",
      "Getting data..\n",
      "Rows: 114830\n",
      "\n",
      "Preparing data..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "043bc8b2e6f24155a565351c41b8c5a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating dataset..\n",
      "Train dataset: 70041, Val dataset: 30468, Test dataset: 14265\n",
      "\n",
      "Defining & Training model..\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 7, 61)]           0         \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 7, 64)             32256     \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 7, 128)            98816     \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 896)               0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 896)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                57408     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 188,545\n",
      "Trainable params: 188,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: tensorflow datasets are not currently supported for gradient and activation auto-logging\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "274/274 [==============================] - 25s 76ms/step - loss: 0.0064 - val_loss: 0.0037\n",
      "Epoch 2/50\n",
      "274/274 [==============================] - 21s 78ms/step - loss: 0.0034 - val_loss: 0.0025\n",
      "Epoch 3/50\n",
      "274/274 [==============================] - 22s 80ms/step - loss: 0.0023 - val_loss: 0.0018\n",
      "Epoch 4/50\n",
      "274/274 [==============================] - 22s 82ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 5/50\n",
      "274/274 [==============================] - 23s 84ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - 23s 85ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 7/50\n",
      "274/274 [==============================] - 23s 84ms/step - loss: 0.0014 - val_loss: 0.0024\n",
      "Epoch 8/50\n",
      "274/274 [==============================] - 23s 84ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 9/50\n",
      "274/274 [==============================] - 23s 84ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 10/50\n",
      "274/274 [==============================] - 23s 82ms/step - loss: 0.0011 - val_loss: 7.8156e-04\n",
      "Epoch 11/50\n",
      "274/274 [==============================] - 23s 83ms/step - loss: 8.8244e-04 - val_loss: 5.8830e-04\n",
      "Epoch 12/50\n",
      "274/274 [==============================] - 22s 82ms/step - loss: 8.9799e-04 - val_loss: 4.4547e-04\n",
      "Epoch 13/50\n",
      "274/274 [==============================] - 22s 82ms/step - loss: 8.9614e-04 - val_loss: 4.8540e-04\n",
      "Epoch 14/50\n",
      "274/274 [==============================] - 23s 83ms/step - loss: 8.6147e-04 - val_loss: 7.0865e-04\n",
      "Epoch 15/50\n",
      "274/274 [==============================] - 23s 82ms/step - loss: 8.0507e-04 - val_loss: 4.6274e-04\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - 22s 80ms/step - loss: 8.8472e-04 - val_loss: 5.1887e-04\n",
      "Epoch 17/50\n",
      "274/274 [==============================] - 23s 83ms/step - loss: 6.8775e-04 - val_loss: 3.6583e-04\n",
      "Epoch 18/50\n",
      "274/274 [==============================] - 22s 82ms/step - loss: 7.0756e-04 - val_loss: 5.2857e-04\n",
      "Epoch 19/50\n",
      "274/274 [==============================] - 23s 82ms/step - loss: 6.7577e-04 - val_loss: 5.0628e-04\n",
      "Epoch 20/50\n",
      "274/274 [==============================] - 23s 83ms/step - loss: 7.5907e-04 - val_loss: 4.4951e-04\n",
      "Epoch 21/50\n",
      "274/274 [==============================] - 23s 83ms/step - loss: 6.7292e-04 - val_loss: 4.3094e-04\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - 23s 84ms/step - loss: 6.3413e-04 - val_loss: 4.4895e-04\n",
      "Epoch 23/50\n",
      "274/274 [==============================] - 23s 83ms/step - loss: 7.3101e-04 - val_loss: 5.2575e-04\n",
      "Epoch 24/50\n",
      "274/274 [==============================] - 23s 84ms/step - loss: 8.0995e-04 - val_loss: 7.0900e-04\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - 23s 84ms/step - loss: 8.3663e-04 - val_loss: 4.9082e-04\n",
      "Epoch 26/50\n",
      "274/274 [==============================] - 23s 84ms/step - loss: 7.9504e-04 - val_loss: 4.7864e-04\n",
      "Epoch 27/50\n",
      "274/274 [==============================] - 23s 85ms/step - loss: 6.2150e-04 - val_loss: 3.4456e-04\n",
      "Epoch 28/50\n",
      "274/274 [==============================] - 23s 85ms/step - loss: 6.5562e-04 - val_loss: 4.9922e-04\n",
      "Epoch 29/50\n",
      "274/274 [==============================] - 23s 85ms/step - loss: 5.8353e-04 - val_loss: 3.7561e-04\n",
      "Epoch 30/50\n",
      "274/274 [==============================] - 23s 85ms/step - loss: 5.5111e-04 - val_loss: 3.1604e-04\n",
      "Epoch 31/50\n",
      "274/274 [==============================] - 23s 85ms/step - loss: 6.0801e-04 - val_loss: 3.8443e-04\n",
      "Epoch 32/50\n",
      "274/274 [==============================] - 23s 84ms/step - loss: 7.4109e-04 - val_loss: 5.7274e-04\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - 23s 84ms/step - loss: 6.7268e-04 - val_loss: 4.1207e-04\n",
      "Epoch 34/50\n",
      "274/274 [==============================] - 23s 84ms/step - loss: 5.7209e-04 - val_loss: 4.5486e-04\n",
      "Epoch 35/50\n",
      "274/274 [==============================] - 23s 85ms/step - loss: 6.0027e-04 - val_loss: 6.5381e-04\n",
      "Epoch 36/50\n",
      "274/274 [==============================] - 23s 85ms/step - loss: 6.2075e-04 - val_loss: 7.1275e-04\n",
      "Epoch 37/50\n",
      "274/274 [==============================] - 23s 84ms/step - loss: 6.0683e-04 - val_loss: 9.2093e-04\n",
      "Epoch 38/50\n",
      "274/274 [==============================] - 23s 83ms/step - loss: 5.8403e-04 - val_loss: 7.3851e-04\n",
      "Epoch 39/50\n",
      "274/274 [==============================] - 23s 83ms/step - loss: 5.7463e-04 - val_loss: 5.7186e-04\n",
      "Epoch 40/50\n",
      "273/274 [============================>.] - ETA: 0s - loss: 5.7686e-04Restoring model weights from the end of the best epoch: 30.\n",
      "274/274 [==============================] - 23s 83ms/step - loss: 5.7617e-04 - val_loss: 9.5666e-04\n",
      "Epoch 40: early stopping\n",
      "56/56 [==============================] - 3s 26ms/step\n",
      "Final Error: 0.012103816193826281\n",
      "Saving Model on ../../models/season/LSTM_base-Batch256-SeqLen7-LossADAM_MSE-Cluster4 ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/season/LSTM_base-Batch256-SeqLen7-LossADAM_MSE-Cluster4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/season/LSTM_base-Batch256-SeqLen7-LossADAM_MSE-Cluster4/assets\n",
      "COMET INFO: ---------------------------------------------------------------------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------------------------------------------------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.com/pveronesi/arpae-tsd-seasonal-experiments/64bee0bab46b44b99e41ae40557ed850\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     batch_loss [1120]         : (0.0001432114077033475, 0.029628317803144455)\n",
      "COMET INFO:     epoch_duration [40]       : (21.27178060099959, 24.676422077999632)\n",
      "COMET INFO:     loss [40]                 : (0.0005511066992767155, 0.006396587938070297)\n",
      "COMET INFO:     val_loss [40]             : (0.0003160409687552601, 0.0037441826425492764)\n",
      "COMET INFO:     validate_batch_loss [480] : (5.630420127999969e-05, 0.0038459389470517635)\n",
      "COMET INFO:   Others:\n",
      "COMET INFO:     Name             : LSTM_base-Batch256-SeqLen7-LossADAM_MSE-Cluster4\n",
      "COMET INFO:     test-error       : 0.012103816193826281\n",
      "COMET INFO:     trainable_params : 188545\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     Adam_amsgrad                 : False\n",
      "COMET INFO:     Adam_beta_1                  : 0.9\n",
      "COMET INFO:     Adam_beta_2                  : 0.999\n",
      "COMET INFO:     Adam_clipnorm                : 1\n",
      "COMET INFO:     Adam_clipvalue               : 1\n",
      "COMET INFO:     Adam_ema_momentum            : 0.99\n",
      "COMET INFO:     Adam_ema_overwrite_frequency : 1\n",
      "COMET INFO:     Adam_epsilon                 : 1e-07\n",
      "COMET INFO:     Adam_global_clipnorm         : 1\n",
      "COMET INFO:     Adam_is_legacy_optimizer     : False\n",
      "COMET INFO:     Adam_jit_compile             : False\n",
      "COMET INFO:     Adam_learning_rate           : 0.0010000000474974513\n",
      "COMET INFO:     Adam_name                    : Adam\n",
      "COMET INFO:     Adam_use_ema                 : False\n",
      "COMET INFO:     Adam_weight_decay            : 1\n",
      "COMET INFO:     epochs                       : 50\n",
      "COMET INFO:     steps                        : 274\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     environment details      : 1\n",
      "COMET INFO:     filename                 : 1\n",
      "COMET INFO:     git metadata             : 1\n",
      "COMET INFO:     git-patch (uncompressed) : 1 (272.95 KB)\n",
      "COMET INFO:     histogram3d              : 410\n",
      "COMET INFO:     installed packages       : 1\n",
      "COMET INFO:     model graph              : 1\n",
      "COMET INFO:     model-element            : 5 (3.65 MB)\n",
      "COMET INFO:     notebook                 : 1\n",
      "COMET INFO:     source_code              : 1\n",
      "COMET INFO: \n",
      "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: sklearn. Metrics and hyperparameters can still be logged using Experiment.log_metrics() and Experiment.log_parameters()\n",
      "COMET INFO: Uploading 1 metrics, params and output messages\n",
      "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: sklearn. Metrics and hyperparameters can still be logged using Experiment.log_metrics() and Experiment.log_parameters()\n",
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Running Cluster_id: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Experiment is live on comet.com https://www.comet.com/pveronesi/arpae-tsd-seasonal-experiments/06e087eeeaf3464ba69c5604a35ef510\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Experiment LSTM_base-Batch512-SeqLen14-LossADAM_MSE-Cluster5:\n",
      "\n",
      "Getting data..\n",
      "Rows: 113613\n",
      "\n",
      "Preparing data..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a093a82080534a00b66b67d8014a9fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating dataset..\n",
      "Train dataset: 68233, Val dataset: 30139, Test dataset: 15241\n",
      "\n",
      "Defining & Training model..\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 14, 62)]          0         \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 14, 256)           326656    \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 14, 128)           197120    \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 1792)              0         \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 1792)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               229504    \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 753,409\n",
      "Trainable params: 753,409\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: tensorflow datasets are not currently supported for gradient and activation auto-logging\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "134/134 [==============================] - 66s 467ms/step - loss: 1.2535 - val_loss: 0.0092\n",
      "Epoch 2/50\n",
      "134/134 [==============================] - 65s 487ms/step - loss: 0.1610 - val_loss: 0.0093\n",
      "Epoch 3/50\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0734 - val_loss: 0.0094\n",
      "Epoch 4/50\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0444 - val_loss: 0.0094\n",
      "Epoch 5/50\n",
      "134/134 [==============================] - 67s 498ms/step - loss: 0.0303 - val_loss: 0.0094\n",
      "Epoch 6/50\n",
      "134/134 [==============================] - 66s 494ms/step - loss: 0.0229 - val_loss: 0.0094\n",
      "Epoch 7/50\n",
      "134/134 [==============================] - 66s 491ms/step - loss: 0.0184 - val_loss: 0.0094\n",
      "Epoch 8/50\n",
      "134/134 [==============================] - 66s 492ms/step - loss: 0.0158 - val_loss: 0.0094\n",
      "Epoch 9/50\n",
      "134/134 [==============================] - 66s 491ms/step - loss: 0.0140 - val_loss: 0.0094\n",
      "Epoch 10/50\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0129 - val_loss: 0.0094\n",
      "Epoch 11/50\n",
      "134/134 [==============================] - ETA: 0s - loss: 0.0121Restoring model weights from the end of the best epoch: 1.\n",
      "134/134 [==============================] - 70s 520ms/step - loss: 0.0121 - val_loss: 0.0093\n",
      "Epoch 11: early stopping\n",
      "30/30 [==============================] - 6s 152ms/step\n",
      "Final Error: 0.07021686832430904\n",
      "Saving Model on ../../models/season/LSTM_base-Batch512-SeqLen14-LossADAM_MSE-Cluster5 ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, lstm_cell_6_layer_call_fn, lstm_cell_6_layer_call_and_return_conditional_losses, lstm_cell_7_layer_call_fn, lstm_cell_7_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/season/LSTM_base-Batch512-SeqLen14-LossADAM_MSE-Cluster5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/season/LSTM_base-Batch512-SeqLen14-LossADAM_MSE-Cluster5/assets\n",
      "COMET INFO: ---------------------------------------------------------------------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------------------------------------------------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.com/pveronesi/arpae-tsd-seasonal-experiments/06e087eeeaf3464ba69c5604a35ef510\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     batch_loss [154]         : (0.00656101992353797, 6.095600605010986)\n",
      "COMET INFO:     epoch_duration [11]      : (65.11013368600015, 69.45805540699985)\n",
      "COMET INFO:     loss [11]                : (0.012071524746716022, 1.2534706592559814)\n",
      "COMET INFO:     val_loss [11]            : (0.009243627078831196, 0.009425985626876354)\n",
      "COMET INFO:     validate_batch_loss [66] : (0.0045441677793860435, 0.009528768248856068)\n",
      "COMET INFO:   Others:\n",
      "COMET INFO:     Name             : LSTM_base-Batch512-SeqLen14-LossADAM_MSE-Cluster5\n",
      "COMET INFO:     test-error       : 0.07021686832430904\n",
      "COMET INFO:     trainable_params : 753409\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     Adam_amsgrad                 : False\n",
      "COMET INFO:     Adam_beta_1                  : 0.9\n",
      "COMET INFO:     Adam_beta_2                  : 0.999\n",
      "COMET INFO:     Adam_clipnorm                : 1\n",
      "COMET INFO:     Adam_clipvalue               : 1\n",
      "COMET INFO:     Adam_ema_momentum            : 0.99\n",
      "COMET INFO:     Adam_ema_overwrite_frequency : 1\n",
      "COMET INFO:     Adam_epsilon                 : 1e-07\n",
      "COMET INFO:     Adam_global_clipnorm         : 1\n",
      "COMET INFO:     Adam_is_legacy_optimizer     : False\n",
      "COMET INFO:     Adam_jit_compile             : False\n",
      "COMET INFO:     Adam_learning_rate           : 0.0010000000474974513\n",
      "COMET INFO:     Adam_name                    : Adam\n",
      "COMET INFO:     Adam_use_ema                 : False\n",
      "COMET INFO:     Adam_weight_decay            : 1\n",
      "COMET INFO:     epochs                       : 50\n",
      "COMET INFO:     steps                        : 134\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     environment details      : 1\n",
      "COMET INFO:     filename                 : 1\n",
      "COMET INFO:     git metadata             : 1\n",
      "COMET INFO:     git-patch (uncompressed) : 1 (277.91 KB)\n",
      "COMET INFO:     histogram3d              : 120\n",
      "COMET INFO:     installed packages       : 1\n",
      "COMET INFO:     model graph              : 1\n",
      "COMET INFO:     model-element            : 5 (10.12 MB)\n",
      "COMET INFO:     notebook                 : 1\n",
      "COMET INFO:     source_code              : 1\n",
      "COMET INFO: \n",
      "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: sklearn. Metrics and hyperparameters can still be logged using Experiment.log_metrics() and Experiment.log_parameters()\n",
      "COMET INFO: Uploading 1 metrics, params and output messages\n",
      "COMET INFO: Waiting for completion of the file uploads (may take several seconds)\n",
      "COMET INFO: The Python SDK has 10800 seconds to finish uploading collected data\n",
      "COMET INFO: Still uploading 2 file(s), remaining 7.30 MB/10.59 MB\n",
      "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: sklearn. Metrics and hyperparameters can still be logged using Experiment.log_metrics() and Experiment.log_parameters()\n",
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Running Cluster_id: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Experiment is live on comet.com https://www.comet.com/pveronesi/arpae-tsd-seasonal-experiments/2a8d79db40c7470fb8b15c41bce4d708\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Experiment LSTM_base-Batch1024-SeqLen7-LossADAM_MSE-Cluster6:\n",
      "\n",
      "Getting data..\n",
      "Rows: 63471\n",
      "\n",
      "Preparing data..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9855da2dae8438cb0ee864bcd883ebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating dataset..\n",
      "Train dataset: 37671, Val dataset: 17037, Test dataset: 8738\n",
      "\n",
      "Defining & Training model..\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 7, 57)]           0         \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               (None, 7, 128)            95232     \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 7, 64)             49408     \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 448)               0         \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 448)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                28736     \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 173,441\n",
      "Trainable params: 173,441\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: tensorflow datasets are not currently supported for gradient and activation auto-logging\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "37/37 [==============================] - 14s 267ms/step - loss: 0.0236 - val_loss: 0.0045\n",
      "Epoch 2/50\n",
      "37/37 [==============================] - 9s 249ms/step - loss: 0.0087 - val_loss: 0.0042\n",
      "Epoch 3/50\n",
      "37/37 [==============================] - 9s 252ms/step - loss: 0.0065 - val_loss: 0.0035\n",
      "Epoch 4/50\n",
      "37/37 [==============================] - 9s 254ms/step - loss: 0.0058 - val_loss: 0.0036\n",
      "Epoch 5/50\n",
      "37/37 [==============================] - 10s 264ms/step - loss: 0.0053 - val_loss: 0.0044\n",
      "Epoch 6/50\n",
      "37/37 [==============================] - 10s 262ms/step - loss: 0.0051 - val_loss: 0.0032\n",
      "Epoch 7/50\n",
      "37/37 [==============================] - 10s 281ms/step - loss: 0.0047 - val_loss: 0.0032\n",
      "Epoch 8/50\n",
      "37/37 [==============================] - 11s 299ms/step - loss: 0.0046 - val_loss: 0.0037\n",
      "Epoch 9/50\n",
      "37/37 [==============================] - 11s 296ms/step - loss: 0.0046 - val_loss: 0.0035\n",
      "Epoch 10/50\n",
      "37/37 [==============================] - 10s 279ms/step - loss: 0.0044 - val_loss: 0.0039\n",
      "Epoch 11/50\n",
      "37/37 [==============================] - 10s 277ms/step - loss: 0.0043 - val_loss: 0.0035\n",
      "Epoch 12/50\n",
      "37/37 [==============================] - 10s 278ms/step - loss: 0.0042 - val_loss: 0.0036\n",
      "Epoch 13/50\n",
      "37/37 [==============================] - 10s 271ms/step - loss: 0.0040 - val_loss: 0.0038\n",
      "Epoch 14/50\n",
      "37/37 [==============================] - 10s 277ms/step - loss: 0.0041 - val_loss: 0.0037\n",
      "Epoch 15/50\n",
      "37/37 [==============================] - 10s 278ms/step - loss: 0.0040 - val_loss: 0.0038\n",
      "Epoch 16/50\n",
      "37/37 [==============================] - 10s 276ms/step - loss: 0.0039 - val_loss: 0.0028\n",
      "Epoch 17/50\n",
      "37/37 [==============================] - 10s 283ms/step - loss: 0.0038 - val_loss: 0.0040\n",
      "Epoch 18/50\n",
      "37/37 [==============================] - 10s 280ms/step - loss: 0.0039 - val_loss: 0.0040\n",
      "Epoch 19/50\n",
      "37/37 [==============================] - 10s 277ms/step - loss: 0.0036 - val_loss: 0.0032\n",
      "Epoch 20/50\n",
      "37/37 [==============================] - 10s 273ms/step - loss: 0.0036 - val_loss: 0.0028\n",
      "Epoch 21/50\n",
      "37/37 [==============================] - 10s 276ms/step - loss: 0.0036 - val_loss: 0.0029\n",
      "Epoch 22/50\n",
      "37/37 [==============================] - 10s 277ms/step - loss: 0.0035 - val_loss: 0.0031\n",
      "Epoch 23/50\n",
      "37/37 [==============================] - 10s 279ms/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 24/50\n",
      "37/37 [==============================] - 10s 279ms/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 25/50\n",
      "37/37 [==============================] - 11s 283ms/step - loss: 0.0033 - val_loss: 0.0027\n",
      "Epoch 26/50\n",
      "37/37 [==============================] - 10s 278ms/step - loss: 0.0034 - val_loss: 0.0025\n",
      "Epoch 27/50\n",
      "37/37 [==============================] - 11s 283ms/step - loss: 0.0033 - val_loss: 0.0025\n",
      "Epoch 28/50\n",
      "37/37 [==============================] - 10s 280ms/step - loss: 0.0032 - val_loss: 0.0028\n",
      "Epoch 29/50\n",
      "37/37 [==============================] - 10s 276ms/step - loss: 0.0031 - val_loss: 0.0025\n",
      "Epoch 30/50\n",
      "37/37 [==============================] - 10s 281ms/step - loss: 0.0030 - val_loss: 0.0025\n",
      "Epoch 31/50\n",
      "37/37 [==============================] - 11s 286ms/step - loss: 0.0029 - val_loss: 0.0023\n",
      "Epoch 32/50\n",
      "37/37 [==============================] - 11s 286ms/step - loss: 0.0028 - val_loss: 0.0021\n",
      "Epoch 33/50\n",
      "37/37 [==============================] - 11s 285ms/step - loss: 0.0028 - val_loss: 0.0027\n",
      "Epoch 34/50\n",
      "37/37 [==============================] - 11s 286ms/step - loss: 0.0027 - val_loss: 0.0024\n",
      "Epoch 35/50\n",
      "37/37 [==============================] - 11s 290ms/step - loss: 0.0027 - val_loss: 0.0020\n",
      "Epoch 36/50\n",
      "37/37 [==============================] - 10s 280ms/step - loss: 0.0028 - val_loss: 0.0026\n",
      "Epoch 37/50\n",
      "37/37 [==============================] - 10s 278ms/step - loss: 0.0028 - val_loss: 0.0026\n",
      "Epoch 38/50\n",
      "37/37 [==============================] - 10s 280ms/step - loss: 0.0028 - val_loss: 0.0022\n",
      "Epoch 39/50\n",
      "37/37 [==============================] - 10s 277ms/step - loss: 0.0028 - val_loss: 0.0023\n",
      "Epoch 40/50\n",
      "37/37 [==============================] - 11s 285ms/step - loss: 0.0027 - val_loss: 0.0024\n",
      "Epoch 41/50\n",
      "37/37 [==============================] - 10s 281ms/step - loss: 0.0027 - val_loss: 0.0024\n",
      "Epoch 42/50\n",
      "37/37 [==============================] - 10s 279ms/step - loss: 0.0027 - val_loss: 0.0020\n",
      "Epoch 43/50\n",
      "37/37 [==============================] - 10s 278ms/step - loss: 0.0027 - val_loss: 0.0022\n",
      "Epoch 44/50\n",
      "37/37 [==============================] - 11s 285ms/step - loss: 0.0027 - val_loss: 0.0025\n",
      "Epoch 45/50\n",
      "37/37 [==============================] - 10s 277ms/step - loss: 0.0027 - val_loss: 0.0021\n",
      "Epoch 46/50\n",
      "37/37 [==============================] - 10s 277ms/step - loss: 0.0026 - val_loss: 0.0020\n",
      "Epoch 47/50\n",
      "37/37 [==============================] - 10s 279ms/step - loss: 0.0027 - val_loss: 0.0025\n",
      "Epoch 48/50\n",
      "37/37 [==============================] - 13s 341ms/step - loss: 0.0026 - val_loss: 0.0022\n",
      "Epoch 49/50\n",
      "37/37 [==============================] - 18s 494ms/step - loss: 0.0026 - val_loss: 0.0020\n",
      "Epoch 50/50\n",
      "37/37 [==============================] - 14s 369ms/step - loss: 0.0027 - val_loss: 0.0021\n",
      "9/9 [==============================] - 3s 102ms/step\n",
      "Final Error: 0.03146629764915405\n",
      "Saving Model on ../../models/season/LSTM_base-Batch1024-SeqLen7-LossADAM_MSE-Cluster6 ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, lstm_cell_8_layer_call_fn, lstm_cell_8_layer_call_and_return_conditional_losses, lstm_cell_9_layer_call_fn, lstm_cell_9_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/season/LSTM_base-Batch1024-SeqLen7-LossADAM_MSE-Cluster6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/season/LSTM_base-Batch1024-SeqLen7-LossADAM_MSE-Cluster6/assets\n",
      "COMET INFO: ---------------------------------------------------------------------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------------------------------------------------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.com/pveronesi/arpae-tsd-seasonal-experiments/2a8d79db40c7470fb8b15c41bce4d708\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     batch_loss [200]          : (0.001027548685669899, 0.061471763998270035)\n",
      "COMET INFO:     epoch_duration [50]       : (9.186701745000391, 18.155133707999994)\n",
      "COMET INFO:     loss [50]                 : (0.0025746540632098913, 0.02362043410539627)\n",
      "COMET INFO:     val_loss [50]             : (0.0019583548419177532, 0.004527262412011623)\n",
      "COMET INFO:     validate_batch_loss [100] : (0.0007260293932631612, 0.004818564746528864)\n",
      "COMET INFO:   Others:\n",
      "COMET INFO:     Name             : LSTM_base-Batch1024-SeqLen7-LossADAM_MSE-Cluster6\n",
      "COMET INFO:     test-error       : 0.03146629764915405\n",
      "COMET INFO:     trainable_params : 173441\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     Adam_amsgrad                 : False\n",
      "COMET INFO:     Adam_beta_1                  : 0.9\n",
      "COMET INFO:     Adam_beta_2                  : 0.999\n",
      "COMET INFO:     Adam_clipnorm                : 1\n",
      "COMET INFO:     Adam_clipvalue               : 1\n",
      "COMET INFO:     Adam_ema_momentum            : 0.99\n",
      "COMET INFO:     Adam_ema_overwrite_frequency : 1\n",
      "COMET INFO:     Adam_epsilon                 : 1e-07\n",
      "COMET INFO:     Adam_global_clipnorm         : 1\n",
      "COMET INFO:     Adam_is_legacy_optimizer     : False\n",
      "COMET INFO:     Adam_jit_compile             : False\n",
      "COMET INFO:     Adam_learning_rate           : 0.0010000000474974513\n",
      "COMET INFO:     Adam_name                    : Adam\n",
      "COMET INFO:     Adam_use_ema                 : False\n",
      "COMET INFO:     Adam_weight_decay            : 1\n",
      "COMET INFO:     epochs                       : 50\n",
      "COMET INFO:     steps                        : 37\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     environment details      : 1\n",
      "COMET INFO:     filename                 : 1\n",
      "COMET INFO:     git metadata             : 1\n",
      "COMET INFO:     git-patch (uncompressed) : 1 (289.24 KB)\n",
      "COMET INFO:     histogram3d              : 510\n",
      "COMET INFO:     installed packages       : 1\n",
      "COMET INFO:     model graph              : 1\n",
      "COMET INFO:     model-element            : 5 (3.47 MB)\n",
      "COMET INFO:     notebook                 : 1\n",
      "COMET INFO:     source_code              : 1\n",
      "COMET INFO: \n",
      "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: sklearn. Metrics and hyperparameters can still be logged using Experiment.log_metrics() and Experiment.log_parameters()\n",
      "COMET INFO: Uploading 1 metrics, params and output messages\n",
      "COMET INFO: Waiting for completion of the file uploads (may take several seconds)\n",
      "COMET INFO: The Python SDK has 10800 seconds to finish uploading collected data\n",
      "COMET INFO: All files uploaded, waiting for confirmation they have been all received\n",
      "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: sklearn. Metrics and hyperparameters can still be logged using Experiment.log_metrics() and Experiment.log_parameters()\n",
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Running Cluster_id: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Experiment is live on comet.com https://www.comet.com/pveronesi/arpae-tsd-seasonal-experiments/73f9bb0bbbee4b1da7af421aac393d3e\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Experiment LSTM_base-Batch256-SeqLen21-LossADAM_MSE-Cluster7:\n",
      "\n",
      "Getting data..\n",
      "Rows: 44946\n",
      "\n",
      "Preparing data..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d11438e96f4b088824c9132e674bf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating dataset..\n",
      "Train dataset: 26671, Val dataset: 12119, Test dataset: 6156\n",
      "\n",
      "Defining & Training model..\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 21, 54)]          0         \n",
      "                                                                 \n",
      " lstm_10 (LSTM)              (None, 21, 256)           318464    \n",
      "                                                                 \n",
      " lstm_11 (LSTM)              (None, 21, 256)           525312    \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 5376)              0         \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 5376)              0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 512)               2753024   \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,597,313\n",
      "Trainable params: 3,597,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: tensorflow datasets are not currently supported for gradient and activation auto-logging\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "105/105 [==============================] - 80s 729ms/step - loss: 0.9583 - val_loss: 0.1550\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - 68s 646ms/step - loss: 0.0512 - val_loss: 0.0531\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - 71s 679ms/step - loss: 0.0308 - val_loss: 0.0244\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - 72s 685ms/step - loss: 0.0188 - val_loss: 0.0231\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - 63s 604ms/step - loss: 0.0190 - val_loss: 0.0185\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - 63s 600ms/step - loss: 0.0165 - val_loss: 0.0207\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - 65s 617ms/step - loss: 0.0190 - val_loss: 0.0127\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - 63s 606ms/step - loss: 0.0135 - val_loss: 0.0132\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - 77s 739ms/step - loss: 0.0133 - val_loss: 0.0167\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - 64s 607ms/step - loss: 0.0110 - val_loss: 0.0108\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - 62s 596ms/step - loss: 0.0107 - val_loss: 0.0094\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - 63s 600ms/step - loss: 0.0081 - val_loss: 0.0065\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - 73s 700ms/step - loss: 0.0076 - val_loss: 0.0083\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - 76s 722ms/step - loss: 0.0073 - val_loss: 0.0052\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - 63s 604ms/step - loss: 0.0070 - val_loss: 0.0043\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - 77s 735ms/step - loss: 0.0066 - val_loss: 0.0052\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - 81s 777ms/step - loss: 0.0067 - val_loss: 0.0042\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - 73s 693ms/step - loss: 0.0063 - val_loss: 0.0043\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - 84s 801ms/step - loss: 0.0060 - val_loss: 0.0033\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - 77s 736ms/step - loss: 0.0058 - val_loss: 0.0033\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - 74s 711ms/step - loss: 0.0057 - val_loss: 0.0033\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - 78s 743ms/step - loss: 0.0057 - val_loss: 0.0035\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - 86s 824ms/step - loss: 0.0057 - val_loss: 0.0033\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - 72s 689ms/step - loss: 0.0053 - val_loss: 0.0056\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - 74s 711ms/step - loss: 0.0061 - val_loss: 0.0051\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - 75s 712ms/step - loss: 0.0063 - val_loss: 0.0054\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - 70s 672ms/step - loss: 0.0064 - val_loss: 0.0057\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - 78s 743ms/step - loss: 0.0062 - val_loss: 0.0062\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - 81s 771ms/step - loss: 0.0058 - val_loss: 0.0041\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - 74s 704ms/step - loss: 0.0054 - val_loss: 0.0031\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - 70s 665ms/step - loss: 0.0052 - val_loss: 0.0033\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - 66s 635ms/step - loss: 0.0052 - val_loss: 0.0031\n",
      "Epoch 33/50\n",
      "105/105 [==============================] - 75s 714ms/step - loss: 0.0052 - val_loss: 0.0036\n",
      "Epoch 34/50\n",
      "105/105 [==============================] - 80s 768ms/step - loss: 0.0053 - val_loss: 0.0039\n",
      "Epoch 35/50\n",
      "105/105 [==============================] - 77s 738ms/step - loss: 0.0055 - val_loss: 0.0050\n",
      "Epoch 36/50\n",
      "105/105 [==============================] - 82s 787ms/step - loss: 0.0054 - val_loss: 0.0069\n",
      "Epoch 37/50\n",
      "105/105 [==============================] - 82s 780ms/step - loss: 0.0055 - val_loss: 0.0055\n",
      "Epoch 38/50\n",
      "105/105 [==============================] - 81s 773ms/step - loss: 0.0052 - val_loss: 0.0054\n",
      "Epoch 39/50\n",
      "105/105 [==============================] - 76s 727ms/step - loss: 0.0049 - val_loss: 0.0040\n",
      "Epoch 40/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.0050Restoring model weights from the end of the best epoch: 30.\n",
      "105/105 [==============================] - 70s 672ms/step - loss: 0.0050 - val_loss: 0.0039\n",
      "Epoch 40: early stopping\n",
      "24/24 [==============================] - 5s 166ms/step\n",
      "Final Error: 0.04104759630334941\n",
      "Saving Model on ../../models/season/LSTM_base-Batch256-SeqLen21-LossADAM_MSE-Cluster7 ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, lstm_cell_10_layer_call_fn, lstm_cell_10_layer_call_and_return_conditional_losses, lstm_cell_11_layer_call_fn, lstm_cell_11_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/season/LSTM_base-Batch256-SeqLen21-LossADAM_MSE-Cluster7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/season/LSTM_base-Batch256-SeqLen21-LossADAM_MSE-Cluster7/assets\n",
      "COMET INFO: ---------------------------------------------------------------------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------------------------------------------------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.com/pveronesi/arpae-tsd-seasonal-experiments/73f9bb0bbbee4b1da7af421aac393d3e\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     batch_loss [440]          : (0.0024038776755332947, 6.734922409057617)\n",
      "COMET INFO:     epoch_duration [40]       : (61.645707894000225, 85.48298803600119)\n",
      "COMET INFO:     loss [40]                 : (0.004888131283223629, 0.9582972526550293)\n",
      "COMET INFO:     val_loss [40]             : (0.003133289748802781, 0.15500809252262115)\n",
      "COMET INFO:     validate_batch_loss [200] : (0.0010001653572544456, 0.1946168839931488)\n",
      "COMET INFO:   Others:\n",
      "COMET INFO:     Name             : LSTM_base-Batch256-SeqLen21-LossADAM_MSE-Cluster7\n",
      "COMET INFO:     test-error       : 0.04104759630334941\n",
      "COMET INFO:     trainable_params : 3597313\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     Adam_amsgrad                 : False\n",
      "COMET INFO:     Adam_beta_1                  : 0.9\n",
      "COMET INFO:     Adam_beta_2                  : 0.999\n",
      "COMET INFO:     Adam_clipnorm                : 1\n",
      "COMET INFO:     Adam_clipvalue               : 1\n",
      "COMET INFO:     Adam_ema_momentum            : 0.99\n",
      "COMET INFO:     Adam_ema_overwrite_frequency : 1\n",
      "COMET INFO:     Adam_epsilon                 : 1e-07\n",
      "COMET INFO:     Adam_global_clipnorm         : 1\n",
      "COMET INFO:     Adam_is_legacy_optimizer     : False\n",
      "COMET INFO:     Adam_jit_compile             : False\n",
      "COMET INFO:     Adam_learning_rate           : 0.0010000000474974513\n",
      "COMET INFO:     Adam_name                    : Adam\n",
      "COMET INFO:     Adam_use_ema                 : False\n",
      "COMET INFO:     Adam_weight_decay            : 1\n",
      "COMET INFO:     epochs                       : 50\n",
      "COMET INFO:     steps                        : 105\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     environment details      : 1\n",
      "COMET INFO:     filename                 : 1\n",
      "COMET INFO:     git metadata             : 1\n",
      "COMET INFO:     git-patch (uncompressed) : 1 (302.64 KB)\n",
      "COMET INFO:     histogram3d              : 410\n",
      "COMET INFO:     installed packages       : 1\n",
      "COMET INFO:     model graph              : 1\n",
      "COMET INFO:     model-element            : 5 (42.67 MB)\n",
      "COMET INFO:     notebook                 : 1\n",
      "COMET INFO:     source_code              : 1\n",
      "COMET INFO: \n",
      "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: sklearn. Metrics and hyperparameters can still be logged using Experiment.log_metrics() and Experiment.log_parameters()\n",
      "COMET INFO: Uploading 1 metrics, params and output messages\n",
      "COMET INFO: Waiting for completion of the file uploads (may take several seconds)\n",
      "COMET INFO: The Python SDK has 10800 seconds to finish uploading collected data\n",
      "COMET INFO: Still uploading 2 file(s), remaining 41.49 MB/43.76 MB\n",
      "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: sklearn. Metrics and hyperparameters can still be logged using Experiment.log_metrics() and Experiment.log_parameters()\n",
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Running Cluster_id: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Experiment is live on comet.com https://www.comet.com/pveronesi/arpae-tsd-seasonal-experiments/7078e5effb594b95b8f143f3273da513\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Experiment LSTM_base-Batch512-SeqLen21-LossADAM_MSE-Cluster8:\n",
      "\n",
      "Getting data..\n",
      "Rows: 86284\n",
      "\n",
      "Preparing data..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a254db32024048f88e94d40e92a59262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating dataset..\n",
      "Train dataset: 54201, Val dataset: 21543, Test dataset: 10540\n",
      "\n",
      "Defining & Training model..\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 21, 63)]          0         \n",
      "                                                                 \n",
      " lstm_12 (LSTM)              (None, 21, 256)           327680    \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 21, 512)           1574912   \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 10752)             0         \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 10752)             0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 128)               1376384   \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,279,105\n",
      "Trainable params: 3,279,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: tensorflow datasets are not currently supported for gradient and activation auto-logging\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "106/106 [==============================] - ETA: 0s - loss: nan"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.comet.com', port=443): Read timed out. (read timeout=10)\")': /clientlib/batch/logger/experiment/metric\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 210s 2s/step - loss: nan - val_loss: nan\n",
      "Epoch 2/50\n",
      "106/106 [==============================] - ETA: 0s - loss: nan"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 197s 2s/step - loss: nan - val_loss: nan\n",
      "Epoch 3/50\n",
      "106/106 [==============================] - ETA: 0s - loss: nan"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 246s 2s/step - loss: nan - val_loss: nan\n",
      "Epoch 4/50\n",
      "106/106 [==============================] - ETA: 0s - loss: nan"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 318s 3s/step - loss: nan - val_loss: nan\n",
      "Epoch 5/50\n",
      "106/106 [==============================] - ETA: 0s - loss: nan"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 260s 2s/step - loss: nan - val_loss: nan\n",
      "Epoch 6/50\n",
      "106/106 [==============================] - ETA: 0s - loss: nan"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 253s 2s/step - loss: nan - val_loss: nan\n",
      "Epoch 7/50\n",
      "106/106 [==============================] - ETA: 0s - loss: nan"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 243s 2s/step - loss: nan - val_loss: nan\n",
      "Epoch 8/50\n",
      "106/106 [==============================] - ETA: 0s - loss: nan"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 245s 2s/step - loss: nan - val_loss: nan\n",
      "Epoch 9/50\n",
      "106/106 [==============================] - ETA: 0s - loss: nan"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 249s 2s/step - loss: nan - val_loss: nan\n",
      "Epoch 10/50\n",
      "106/106 [==============================] - ETA: 0s - loss: nanRestoring model weights from the end of the best epoch: 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n",
      "COMET WARNING: ignoring empty histogram\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 247s 2s/step - loss: nan - val_loss: nan\n",
      "Epoch 10: early stopping\n",
      "21/21 [==============================] - 16s 675ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET ERROR: Error sending log other messages, got 400 b'{\"msg\":\"Invalid json format: Non-standard token \\'NaN\\': enable `JsonReadFeature.ALLOW_NON_NUMERIC_NUMBERS` to allow\\\\n at [Source: (org.glassfish.jersey.message.internal.ReaderInterceptorExecutor$UnCloseableInputStream); line: 1, column: 88]\",\"code\":400,\"data\":null,\"sdk_error_code\":0}'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Error: nan\n",
      "Saving Model on ../../models/season/LSTM_base-Batch512-SeqLen21-LossADAM_MSE-Cluster8 ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, lstm_cell_12_layer_call_fn, lstm_cell_12_layer_call_and_return_conditional_losses, lstm_cell_13_layer_call_fn, lstm_cell_13_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/season/LSTM_base-Batch512-SeqLen21-LossADAM_MSE-Cluster8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/season/LSTM_base-Batch512-SeqLen21-LossADAM_MSE-Cluster8/assets\n",
      "COMET INFO: ---------------------------------------------------------------------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------------------------------------------------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.com/pveronesi/arpae-tsd-seasonal-experiments/7078e5effb594b95b8f143f3273da513\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     batch_loss          : nan\n",
      "COMET INFO:     epoch_duration [10] : (197.28925216899916, 317.5939305840002)\n",
      "COMET INFO:     loss                : nan\n",
      "COMET INFO:     val_loss            : nan\n",
      "COMET INFO:     validate_batch_loss : nan\n",
      "COMET INFO:   Others:\n",
      "COMET INFO:     Name             : LSTM_base-Batch512-SeqLen21-LossADAM_MSE-Cluster8\n",
      "COMET INFO:     test-error       : nan\n",
      "COMET INFO:     trainable_params : 3279105\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     Adam_amsgrad                 : False\n",
      "COMET INFO:     Adam_beta_1                  : 0.9\n",
      "COMET INFO:     Adam_beta_2                  : 0.999\n",
      "COMET INFO:     Adam_clipnorm                : 1\n",
      "COMET INFO:     Adam_clipvalue               : 1\n",
      "COMET INFO:     Adam_ema_momentum            : 0.99\n",
      "COMET INFO:     Adam_ema_overwrite_frequency : 1\n",
      "COMET INFO:     Adam_epsilon                 : 1e-07\n",
      "COMET INFO:     Adam_global_clipnorm         : 1\n",
      "COMET INFO:     Adam_is_legacy_optimizer     : False\n",
      "COMET INFO:     Adam_jit_compile             : False\n",
      "COMET INFO:     Adam_learning_rate           : 0.0010000000474974513\n",
      "COMET INFO:     Adam_name                    : Adam\n",
      "COMET INFO:     Adam_use_ema                 : False\n",
      "COMET INFO:     Adam_weight_decay            : 1\n",
      "COMET INFO:     epochs                       : 50\n",
      "COMET INFO:     steps                        : 106\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     environment details      : 1\n",
      "COMET INFO:     filename                 : 1\n",
      "COMET INFO:     git metadata             : 1\n",
      "COMET INFO:     git-patch (uncompressed) : 1 (316.76 KB)\n",
      "COMET INFO:     histogram3d              : 10\n",
      "COMET INFO:     installed packages       : 1\n",
      "COMET INFO:     model graph              : 1\n",
      "COMET INFO:     model-element            : 5 (39.03 MB)\n",
      "COMET INFO:     notebook                 : 1\n",
      "COMET INFO:     source_code              : 1\n",
      "COMET INFO: \n",
      "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: sklearn. Metrics and hyperparameters can still be logged using Experiment.log_metrics() and Experiment.log_parameters()\n",
      "COMET INFO: Uploading 1 metrics, params and output messages\n",
      "COMET INFO: Waiting for completion of the file uploads (may take several seconds)\n",
      "COMET INFO: The Python SDK has 10800 seconds to finish uploading collected data\n",
      "COMET INFO: Still uploading 1 file(s), remaining 31.83 MB/39.31 MB\n",
      "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: sklearn. Metrics and hyperparameters can still be logged using Experiment.log_metrics() and Experiment.log_parameters()\n",
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Running Cluster_id: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Experiment is live on comet.com https://www.comet.com/pveronesi/arpae-tsd-seasonal-experiments/10c71d557b034447b3401e9b49c4cb4f\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Experiment LSTM_base-Batch1024-SeqLen14-LossADAM_MSE-Cluster9:\n",
      "\n",
      "Getting data..\n",
      "Rows: 33185\n",
      "\n",
      "Preparing data..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c29daa80e4478c82c6596cff2f312e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating dataset..\n",
      "Train dataset: 21099, Val dataset: 8087, Test dataset: 3999\n",
      "\n",
      "Defining & Training model..\n",
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 14, 57)]          0         \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 14, 64)            31232     \n",
      "                                                                 \n",
      " lstm_15 (LSTM)              (None, 14, 64)            33024     \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 896)               0         \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 896)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 128)               114816    \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 179,201\n",
      "Trainable params: 179,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: tensorflow datasets are not currently supported for gradient and activation auto-logging\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "21/21 [==============================] - 13s 405ms/step - loss: 0.0672 - val_loss: 0.0626\n",
      "Epoch 2/50\n",
      "21/21 [==============================] - 9s 404ms/step - loss: 0.0468 - val_loss: 0.0444\n",
      "Epoch 3/50\n",
      "21/21 [==============================] - 9s 413ms/step - loss: 0.0412 - val_loss: 0.0667\n",
      "Epoch 4/50\n",
      "21/21 [==============================] - 9s 445ms/step - loss: 0.0413 - val_loss: 0.0162\n",
      "Epoch 5/50\n",
      "21/21 [==============================] - 9s 418ms/step - loss: 0.0188 - val_loss: 0.0187\n",
      "Epoch 6/50\n",
      "21/21 [==============================] - 8s 378ms/step - loss: 0.0145 - val_loss: 0.0141\n",
      "Epoch 7/50\n",
      "21/21 [==============================] - 8s 377ms/step - loss: 0.0152 - val_loss: 0.0185\n",
      "Epoch 8/50\n",
      "21/21 [==============================] - 10s 450ms/step - loss: 0.0120 - val_loss: 0.0055\n",
      "Epoch 9/50\n",
      "21/21 [==============================] - 9s 435ms/step - loss: 0.0081 - val_loss: 0.0082\n",
      "Epoch 10/50\n",
      "21/21 [==============================] - 8s 369ms/step - loss: 0.0090 - val_loss: 0.0108\n",
      "Epoch 11/50\n",
      "21/21 [==============================] - 7s 351ms/step - loss: 0.0087 - val_loss: 0.0110\n",
      "Epoch 12/50\n",
      "21/21 [==============================] - 7s 349ms/step - loss: 0.0104 - val_loss: 0.0044\n",
      "Epoch 13/50\n",
      "21/21 [==============================] - 7s 348ms/step - loss: 0.0072 - val_loss: 0.0084\n",
      "Epoch 14/50\n",
      "21/21 [==============================] - 7s 349ms/step - loss: 0.0079 - val_loss: 0.0067\n",
      "Epoch 15/50\n",
      "21/21 [==============================] - 8s 365ms/step - loss: 0.0091 - val_loss: 0.0048\n",
      "Epoch 16/50\n",
      "21/21 [==============================] - 8s 361ms/step - loss: 0.0068 - val_loss: 0.0064\n",
      "Epoch 17/50\n",
      "21/21 [==============================] - 7s 343ms/step - loss: 0.0071 - val_loss: 0.0042\n",
      "Epoch 18/50\n",
      "21/21 [==============================] - 7s 349ms/step - loss: 0.0070 - val_loss: 0.0059\n",
      "Epoch 19/50\n",
      "21/21 [==============================] - 8s 370ms/step - loss: 0.0075 - val_loss: 0.0048\n",
      "Epoch 20/50\n",
      "21/21 [==============================] - 9s 415ms/step - loss: 0.0058 - val_loss: 0.0034\n",
      "Epoch 21/50\n",
      "21/21 [==============================] - 9s 441ms/step - loss: 0.0058 - val_loss: 0.0065\n",
      "Epoch 22/50\n",
      "21/21 [==============================] - 9s 408ms/step - loss: 0.0069 - val_loss: 0.0040\n",
      "Epoch 23/50\n",
      "21/21 [==============================] - 9s 447ms/step - loss: 0.0064 - val_loss: 0.0056\n",
      "Epoch 24/50\n",
      "21/21 [==============================] - 10s 457ms/step - loss: 0.0060 - val_loss: 0.0032\n",
      "Epoch 25/50\n",
      "21/21 [==============================] - 9s 435ms/step - loss: 0.0055 - val_loss: 0.0065\n",
      "Epoch 26/50\n",
      "21/21 [==============================] - 9s 440ms/step - loss: 0.0066 - val_loss: 0.0035\n",
      "Epoch 27/50\n",
      "21/21 [==============================] - 9s 442ms/step - loss: 0.0059 - val_loss: 0.0052\n",
      "Epoch 28/50\n",
      "21/21 [==============================] - 9s 412ms/step - loss: 0.0057 - val_loss: 0.0033\n",
      "Epoch 29/50\n",
      "21/21 [==============================] - 9s 440ms/step - loss: 0.0051 - val_loss: 0.0055\n",
      "Epoch 30/50\n",
      "21/21 [==============================] - 10s 472ms/step - loss: 0.0060 - val_loss: 0.0032\n",
      "Epoch 31/50\n",
      "21/21 [==============================] - 8s 396ms/step - loss: 0.0054 - val_loss: 0.0048\n",
      "Epoch 32/50\n",
      "21/21 [==============================] - 9s 407ms/step - loss: 0.0055 - val_loss: 0.0032\n",
      "Epoch 33/50\n",
      "21/21 [==============================] - 9s 419ms/step - loss: 0.0047 - val_loss: 0.0043\n",
      "Epoch 34/50\n",
      "21/21 [==============================] - 8s 399ms/step - loss: 0.0052 - val_loss: 0.0028\n",
      "Epoch 35/50\n",
      "21/21 [==============================] - 8s 386ms/step - loss: 0.0048 - val_loss: 0.0044\n",
      "Epoch 36/50\n",
      "21/21 [==============================] - 8s 360ms/step - loss: 0.0053 - val_loss: 0.0030\n",
      "Epoch 37/50\n",
      "21/21 [==============================] - 7s 343ms/step - loss: 0.0044 - val_loss: 0.0035\n",
      "Epoch 38/50\n",
      "21/21 [==============================] - 8s 398ms/step - loss: 0.0045 - val_loss: 0.0025\n",
      "Epoch 39/50\n",
      "21/21 [==============================] - 9s 413ms/step - loss: 0.0042 - val_loss: 0.0039\n",
      "Epoch 40/50\n",
      "21/21 [==============================] - 8s 396ms/step - loss: 0.0048 - val_loss: 0.0027\n",
      "Epoch 41/50\n",
      "21/21 [==============================] - 8s 384ms/step - loss: 0.0043 - val_loss: 0.0036\n",
      "Epoch 42/50\n",
      "21/21 [==============================] - 8s 396ms/step - loss: 0.0042 - val_loss: 0.0026\n",
      "Epoch 43/50\n",
      "21/21 [==============================] - 8s 366ms/step - loss: 0.0042 - val_loss: 0.0032\n",
      "Epoch 44/50\n",
      "21/21 [==============================] - 7s 350ms/step - loss: 0.0042 - val_loss: 0.0028\n",
      "Epoch 45/50\n",
      "21/21 [==============================] - 8s 398ms/step - loss: 0.0043 - val_loss: 0.0033\n",
      "Epoch 46/50\n",
      "21/21 [==============================] - 8s 362ms/step - loss: 0.0041 - val_loss: 0.0029\n",
      "Epoch 47/50\n",
      "21/21 [==============================] - 7s 350ms/step - loss: 0.0040 - val_loss: 0.0026\n",
      "Epoch 48/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0039Restoring model weights from the end of the best epoch: 38.\n",
      "21/21 [==============================] - 8s 361ms/step - loss: 0.0039 - val_loss: 0.0030\n",
      "Epoch 48: early stopping\n",
      "4/4 [==============================] - 2s 103ms/step\n",
      "Final Error: 0.028978248135867326\n",
      "Saving Model on ../../models/season/LSTM_base-Batch1024-SeqLen14-LossADAM_MSE-Cluster9 ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, lstm_cell_14_layer_call_fn, lstm_cell_14_layer_call_and_return_conditional_losses, lstm_cell_15_layer_call_fn, lstm_cell_15_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/season/LSTM_base-Batch1024-SeqLen14-LossADAM_MSE-Cluster9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/season/LSTM_base-Batch1024-SeqLen14-LossADAM_MSE-Cluster9/assets\n",
      "COMET INFO: ---------------------------------------------------------------------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------------------------------------------------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.com/pveronesi/arpae-tsd-seasonal-experiments/10c71d557b034447b3401e9b49c4cb4f\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     batch_loss [144]         : (0.002576156985014677, 0.14112849533557892)\n",
      "COMET INFO:     epoch_duration [48]      : (7.133007451000594, 13.224025477000396)\n",
      "COMET INFO:     loss [48]                : (0.003886523423716426, 0.06719209253787994)\n",
      "COMET INFO:     val_loss [48]            : (0.0025357070844620466, 0.06669794768095016)\n",
      "COMET INFO:     validate_batch_loss [48] : (0.00259058247320354, 0.17002668976783752)\n",
      "COMET INFO:   Others:\n",
      "COMET INFO:     Name             : LSTM_base-Batch1024-SeqLen14-LossADAM_MSE-Cluster9\n",
      "COMET INFO:     test-error       : 0.028978248135867326\n",
      "COMET INFO:     trainable_params : 179201\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     Adam_amsgrad                 : False\n",
      "COMET INFO:     Adam_beta_1                  : 0.9\n",
      "COMET INFO:     Adam_beta_2                  : 0.999\n",
      "COMET INFO:     Adam_clipnorm                : 1\n",
      "COMET INFO:     Adam_clipvalue               : 1\n",
      "COMET INFO:     Adam_ema_momentum            : 0.99\n",
      "COMET INFO:     Adam_ema_overwrite_frequency : 1\n",
      "COMET INFO:     Adam_epsilon                 : 1e-07\n",
      "COMET INFO:     Adam_global_clipnorm         : 1\n",
      "COMET INFO:     Adam_is_legacy_optimizer     : False\n",
      "COMET INFO:     Adam_jit_compile             : False\n",
      "COMET INFO:     Adam_learning_rate           : 0.0010000000474974513\n",
      "COMET INFO:     Adam_name                    : Adam\n",
      "COMET INFO:     Adam_use_ema                 : False\n",
      "COMET INFO:     Adam_weight_decay            : 1\n",
      "COMET INFO:     epochs                       : 50\n",
      "COMET INFO:     steps                        : 21\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     environment details      : 1\n",
      "COMET INFO:     filename                 : 1\n",
      "COMET INFO:     git metadata             : 1\n",
      "COMET INFO:     git-patch (uncompressed) : 1 (333.61 KB)\n",
      "COMET INFO:     histogram3d              : 490\n",
      "COMET INFO:     installed packages       : 1\n",
      "COMET INFO:     model graph              : 1\n",
      "COMET INFO:     model-element            : 5 (3.54 MB)\n",
      "COMET INFO:     notebook                 : 1\n",
      "COMET INFO:     source_code              : 1\n",
      "COMET INFO: \n",
      "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: sklearn. Metrics and hyperparameters can still be logged using Experiment.log_metrics() and Experiment.log_parameters()\n",
      "COMET INFO: Uploading 1 metrics, params and output messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "if exp_wdgt_value.value == 'Manual Params':\n",
    "    # Set params\n",
    "    TAG = \"\"\n",
    "    sample_cluster_id = DEFAULT_CLUSTER_ID\n",
    "    model_name = DEFAULT_MODEL_NAME\n",
    "    batch_size = DEFAULT_BATCH_SIZE\n",
    "    sequence_len = DEFAULT_SEQUENCE_LEN\n",
    "    learning_rate = DEFAULT_LEARNING_RATE\n",
    "    weight_decay = DEFAULT_WEIGHT_DECAY\n",
    "    rho = DEFAULT_RHO    \n",
    "    loss = DEFAULT_LOSS\n",
    "    epochs = DEFAULT_EPOCHS\n",
    "    dropout = DEFAULT_DROPOUT\n",
    "    layer_lstm_1_units = DEFAULT_LAYER_LSTM_1_UNITS\n",
    "    layer_lstm_2_units = DEFAULT_LAYER_LSTM_2_UNITS\n",
    "    layer_dense_units = DEFAULT_LAYER_DENSE_UNITS    \n",
    "    # Run experiment\n",
    "    history, test_dataset, test_df, preds, error, feats = _run_experiment(COMET_API_KEY, COMET_WORKSPACE, \n",
    "                                                                          COMET_PROJECT_NAME, data_df, \n",
    "                                                                          clusters_df, sample_cluster_id, TAG, \n",
    "                                                                          model_name, batch_size, sequence_len, \n",
    "                                                                          loss, learning_rate, weight_decay, rho, \n",
    "                                                                          epochs, \n",
    "                                                                          dropout, layer_lstm_1_units, \n",
    "                                                                          layer_lstm_2_units, layer_dense_units,\n",
    "                                                                          ORIGINAL_FEATS, METEO_FEATS, \n",
    "                                                                          POLLEN_FEATS, DATE_COL, LABEL_COL, \n",
    "                                                                          TRAIN_END, VAL_END, TEST_END)\n",
    "    # Save results\n",
    "    results[sample_cluster_id] = {\n",
    "        'history': history,\n",
    "        'test_dataset': test_dataset, \n",
    "        'test_df': test_df, \n",
    "        'preds': preds, \n",
    "        'error': error,\n",
    "        'feats': feats\n",
    "    }\n",
    "    print(\"Done.\")\n",
    "    \n",
    "elif exp_wdgt_value.value == 'Opt Params':\n",
    "    # For each cluster\n",
    "    for cluster_id in clusters:\n",
    "        print(\"Running Cluster_id: {}\".format(cluster_id))\n",
    "        TAG = \"OPT\"        \n",
    "        # Get Cluster Opt params\n",
    "        params = params_df[params_df['cluster_id']==cluster_id]\n",
    "        model_name = params['model_name'].values[0]\n",
    "        batch_size = params['batch_size'].values[0]\n",
    "        sequence_len = params['sequence_len'].values[0]\n",
    "        learning_rate = params['learning_rate'].values[0]\n",
    "        weight_decay = DEFAULT_WEIGHT_DECAY\n",
    "        rho = DEFAULT_RHO\n",
    "        loss = DEFAULT_LOSS\n",
    "        epochs = DEFAULT_EPOCHS           \n",
    "        dropout = params['dropout'].values[0]\n",
    "        layer_lstm_1_units = params['layer_lstm_1_units'].values[0]\n",
    "        layer_lstm_2_units = params['layer_lstm_2_units'].values[0]\n",
    "        layer_dense_units = params['layer_dense_units'].values[0]    \n",
    "        # Run experiment\n",
    "        history, test_dataset, test_df, preds, error, feats = _run_experiment(COMET_API_KEY, COMET_WORKSPACE, \n",
    "                                                                              COMET_PROJECT_NAME, data_df, \n",
    "                                                                              clusters_df, cluster_id, TAG,\n",
    "                                                                              model_name, batch_size, \n",
    "                                                                              sequence_len, loss, learning_rate, \n",
    "                                                                              weight_decay, rho, epochs,\n",
    "                                                                              dropout, layer_lstm_1_units, \n",
    "                                                                              layer_lstm_2_units,  \n",
    "                                                                              layer_dense_units,\n",
    "                                                                              ORIGINAL_FEATS, METEO_FEATS, \n",
    "                                                                              POLLEN_FEATS, DATE_COL, LABEL_COL, \n",
    "                                                                              TRAIN_END, VAL_END, TEST_END)\n",
    "        # Save results\n",
    "        results[cluster_id] = {\n",
    "            'history': history,\n",
    "            'test_dataset': test_dataset, \n",
    "            'test_df': test_df, \n",
    "            'preds': preds, \n",
    "            'error': error,\n",
    "            'feats': feats\n",
    "        }\n",
    "        print(\"Done.\")\n",
    "    \n",
    "else:\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac77c477",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d66c45e",
   "metadata": {},
   "source": [
    "<h3>4. Evaluate Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e72e43c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster_id: 3\n",
      "MSE Error: 0.013022032861696715\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6z/h5gl1rs567sb9kzq90zfly6w0000gv/T/ipykernel_6431/778499928.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparams_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cluster_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mcluster_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0msequence_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sequence_len'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0m_feats_importances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_dataset'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'feats'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Plot Preds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/6z/h5gl1rs567sb9kzq90zfly6w0000gv/T/ipykernel_6431/1090771828.py\u001b[0m in \u001b[0;36m_feats_importances\u001b[0;34m(test_dataset, feats, sequence_len)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Get baseline error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mfeats_imp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mff_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mff_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mff_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot History & Preds for each cluster\n",
    "\n",
    "for cluster_id, result in results.items():\n",
    "    print(\"Cluster_id: {}\".format(cluster_id))\n",
    "    print(\"MSE Error: {}\".format(result['error']))\n",
    "    # Plot History\n",
    "    _plot_history(history)\n",
    "\n",
    "    # Plot Feat Importances\n",
    "    params = params_df[params_df['cluster_id']==cluster_id]\n",
    "    sequence_len = params['sequence_len'].values[0]\n",
    "    _feats_importances(result['test_dataset'], result['feats'], sequence_len)\n",
    "\n",
    "    # Plot Preds\n",
    "    _plot_preds(result['preds'], result['test_df'], LABEL_COL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3546ce04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7d55a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3012680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bdb8d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6fde6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74a8a45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b59b69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b77168",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
