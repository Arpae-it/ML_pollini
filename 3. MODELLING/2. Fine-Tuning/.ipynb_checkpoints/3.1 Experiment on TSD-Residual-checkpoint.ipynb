{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27e9f867",
   "metadata": {},
   "source": [
    "# Experiment on Cluster TSD Residual\n",
    "Notebook to perform some model on a single TSD Residual Cluster: trhougth time-series-decomposition, the residual inside the cluster is extracted and used to make prediction.<br>\n",
    "Data read are from table SLIDING_WINDOWS_DATASET that contains a sliding windows of:\n",
    "- feats about last 7-days meteo values\n",
    "- pollen value for the next day\n",
    "\n",
    "Cluster associations are read from a local file: we have different cluster annotations made by different techniques.<br>\n",
    "We explore different model & hyper-parameters throught Comet ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96a293a",
   "metadata": {},
   "source": [
    "<h3>Import</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80880ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from google.cloud import bigquery\n",
    "\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import get_cmap\n",
    "from matplotlib import cm\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "import ipywidgets as widgets\n",
    "import scipy.stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "\n",
    "from comet_ml import Experiment\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers.experimental import Adam, AdamW, Adadelta\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "my_cmap = plt.get_cmap(\"Paired\")\n",
    "init_notebook_mode(connected=True)  \n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7c4047",
   "metadata": {},
   "source": [
    "<h3>Config</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7031950c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "\n",
    "PROJECT_ID = 'arpae-prod-ml'\n",
    "\n",
    "# BigQuery\n",
    "BQ_DATASET = 'SAMPLE_DATA'\n",
    "JOINED_BQ_DATASET = 'JOINED_DATA'\n",
    "\n",
    "# Const\n",
    "COMMON_PERIOD_INIT = '2011-01-01'\n",
    "COMMON_PERIOD_END = '2023-12-31' \n",
    "\n",
    "TRAIN_END = '2016-12-31 00:00:00+00:00'\n",
    "VAL_END = '2019-12-31 00:00:00+00:00'\n",
    "TEST_END = '2022-12-31 00:00:00+00:00'\n",
    "\n",
    "# Cols\n",
    "DATE_COL = 'date'\n",
    "\n",
    "# Feats\n",
    "METEO_FEATS = ['week_amax', \n",
    "               'station_lat_amax', 'station_lon_amax', 'station_H_piano_strada_amax', 'station_H_mslm_amax', \n",
    "               'B13011_min_amin', 'B13011_max_amax', 'B13011_mean_mean', 'B13011_std_mean', 'B13011_sum_sum', \n",
    "               'B14198_min_amin', 'B14198_max_amax', 'B14198_mean_mean', 'B14198_std_mean', 'B14198_sum_sum',\n",
    "               'TEMP_min_amin', 'TEMP_max_amax', 'TEMP_mean_mean', 'TEMP_std_mean', 'TEMP_sum_sum',                                               \n",
    "               'PREC_amin', 'PREC_mean', 'PREC_std', 'PREC_median', 'PREC_amax', 'PREC_skew', 'PREC_kurtosis']\n",
    "POLLEN_FEATS = ['residual_mean', 'residual_prev_1', # seasonal, trend, residual\n",
    "                'pol_value_amin', 'pol_value_mean', 'pol_value_std', 'pol_value_median', 'pol_value_amax', \n",
    "                'pol_value_skew', 'pol_value_kurtosis',\n",
    "                'pol_value_prev_1', 'pol_value_prev_2', 'pol_value_prev_3',\n",
    "                'pol_value_prev_4', 'pol_value_prev_5', 'pol_value_prev_6',\n",
    "                'pol_value_prev_7']\n",
    "ORIGINAL_FEATS = METEO_FEATS + POLLEN_FEATS\n",
    "LABEL_COL = 'residual_label' # season, trend, residual\n",
    "\n",
    "# Params\n",
    "EPOCHS = 50\n",
    "\n",
    "# Comet Params\n",
    "COMET_API_KEY = 'B4Tttbbx4JrwXD9x2HBNjCdXX'\n",
    "COMET_WORKSPACE = 'pveronesi' \n",
    "COMET_PROJECT_NAME = 'arpae-tsd-residual-experiments' # season, trend, residual\n",
    "\n",
    "# Layout\n",
    "COLOR_PALETTE = px.colors.qualitative.Prism\n",
    "\n",
    "OUTPUT_CLUSTER_FILENAME = \"../../data/clustering_residual_intervals.csv\" # season, trend, residual\n",
    "OPT_PARAMS_FILENAME = \"../../data/optimal_params_residual.csv\" # season, trend, residual\n",
    "MODEL_DIR = \"../../models/residual/\" # season, trend, residual\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b40c5c9",
   "metadata": {},
   "source": [
    "<h3>Methods</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccb72ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Methods\n",
    "\n",
    "def _run_query(client, query): \n",
    "    df = client.query(query).to_dataframe()\n",
    "    return df\n",
    "\n",
    "def _read_table(client, project_id, dataset, table):\n",
    "    query = \"SELECT * FROM `{}.{}.{}` \".format(project_id, dataset, table)\n",
    "    df = _run_query(client, query)\n",
    "    return df\n",
    "\n",
    "def _read_table_delta(client, project_id, dataset, table, date_col, init, end):\n",
    "    query = \"SELECT * FROM `{}.{}.{}` WHERE {} > '{}' AND {} < '{}' \".format(project_id, dataset, table, date_col, init, date_col, end)\n",
    "    df = _run_query(client, query)\n",
    "    if 'reftime' in df.columns:\n",
    "        df.sort_values(by='reftime', inplace=True)\n",
    "    elif date_col in df.columns:\n",
    "        df.sort_values(by=date_col, inplace=True)\n",
    "    else:\n",
    "        return None\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45dded16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comet methods\n",
    "\n",
    "def _create_experiment(api_key, workspace, project_name):\n",
    "    experiment = Experiment(\n",
    "        # user config\n",
    "        api_key=api_key,\n",
    "        workspace=workspace,  \n",
    "        # project config\n",
    "        project_name=project_name,\n",
    "        # logging config\n",
    "        log_code=True,\n",
    "        log_graph=True,\n",
    "        auto_param_logging=True,\n",
    "        auto_metric_logging=True,    \n",
    "        auto_histogram_weight_logging=True,\n",
    "        auto_histogram_gradient_logging=True,\n",
    "        auto_histogram_activation_logging=True\n",
    "    )\n",
    "    return experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36411aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Methods\n",
    "\n",
    "def _create_experiment_widget():\n",
    "    exp_wdgt = widgets.Dropdown(options=['Opt Params', 'Manual Params'], description='Set Params:', layout={\"width\":\"50%\"})\n",
    "    return exp_wdgt\n",
    "\n",
    "def _create_cluster_widget(clusters):\n",
    "    cluster_wdgt = widgets.Dropdown(options=clusters, description='Cluster id:', layout={\"width\":\"50%\"})\n",
    "    return cluster_wdgt\n",
    "\n",
    "def _normalize(x, range_dict, index_col, label_col):\n",
    "    pol_min = range_dict[x[index_col]]['min']\n",
    "    pol_max = range_dict[x[index_col]]['max']\n",
    "    return (x[label_col] - pol_min) / (pol_max - pol_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37550651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Methods\n",
    "\n",
    "def _get_data(data_df, clusters_df, cluster_id, feats_cols, date_col, label_col):\n",
    "    # filter data\n",
    "    filt_clusters_df = clusters_df[clusters_df['cluster']==cluster_id][['station_id', 'pol_var_id']]\n",
    "    dataset_df = pd.merge(data_df, filt_clusters_df, how='right', on=['station_id', 'pol_var_id'])\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset_df.sort_values(['station_id', 'pol_var_id', date_col], inplace=True)\n",
    "    dataset_df = dataset_df[['station_id', 'pol_var_id', date_col]  + feats_cols + [label_col]]\n",
    "    \n",
    "    # Set index and drop nan\n",
    "    dataset_df.set_index(date_col, inplace=True)\n",
    "    dataset_df.dropna(inplace=True)\n",
    "    \n",
    "    print(\"Rows: {}\".format(dataset_df.shape[0]))\n",
    "    return dataset_df\n",
    "\n",
    "def _prepare_data(dataset_df, original_feats, meteo_feats, pollen_feats, label_col):\n",
    "    # Add 1-hot encoding cols\n",
    "    stations_one_hot = pd.get_dummies(dataset_df['station_id'], prefix='station_id')\n",
    "    pollen_one_hot = pd.get_dummies(dataset_df['pol_var_id'], prefix='pol_var_id')\n",
    "    dataset_df = pd.concat([dataset_df, stations_one_hot], axis=1)\n",
    "    dataset_df = pd.concat([dataset_df, pollen_one_hot], axis=1)\n",
    "    \n",
    "    # Update Features\n",
    "    feats = original_feats + stations_one_hot.columns.values.tolist() + pollen_one_hot.columns.values.tolist()\n",
    "    n_feats = len(feats)\n",
    "    \n",
    "    # Normalize all cols except for pollen ones; Save max cols to restore original values\n",
    "    cols_max = []\n",
    "    for col in meteo_feats:\n",
    "        scaler = MinMaxScaler()\n",
    "        dataset_df[col] = pd.DataFrame(scaler.fit_transform(dataset_df[[col]])).values\n",
    "        cols_max.append(int(scaler.data_max_))\n",
    "\n",
    "    # Normalize with Min-Max scaling each pollen feats \n",
    "    for col in tqdm(pollen_feats):\n",
    "        range_df = dataset_df[['pol_var_id', col]].groupby('pol_var_id').agg(['min', 'max'])\n",
    "        ranges = {}\n",
    "        for index, values in zip(range_df.index, range_df.values):\n",
    "            ranges[index] = {'min': values[0], 'max': values[1]}        \n",
    "        dataset_df[col] = dataset_df.apply(lambda x: _normalize(x, ranges, 'pol_var_id', col), axis=1)    \n",
    "    \n",
    "    # Normalize with Min-Max scaling the label col\n",
    "    range_df = dataset_df[['pol_var_id', LABEL_COL]].groupby('pol_var_id').agg(['min', 'max'])\n",
    "    ranges = {}\n",
    "    for index, values in zip(range_df.index, range_df.values):\n",
    "        ranges[index] = {'min': values[0], 'max': values[1]}        \n",
    "    dataset_df[label_col] = dataset_df.apply(lambda x: _normalize(x, ranges, 'pol_var_id', label_col), axis=1)    \n",
    "    \n",
    "    # Sort data\n",
    "    dataset_df.index = pd.to_datetime(dataset_df.index)\n",
    "    dataset_df.sort_values(by=['station_id', 'pol_var_id', 'date'], inplace=True)\n",
    "\n",
    "    return dataset_df, feats, n_feats, cols_max\n",
    "\n",
    "def _create_datasets(dataset_df, train_end, val_end, test_end, feats, label_col, batch_size):\n",
    "    # Split df into train and test sets\n",
    "    train_df = dataset_df[dataset_df.index < pd.to_datetime(train_end)]\n",
    "    val_df = dataset_df[(dataset_df.index > pd.to_datetime(train_end)) & \n",
    "                        (dataset_df.index < pd.to_datetime(val_end))]\n",
    "    test_df = dataset_df[(dataset_df.index > pd.to_datetime(val_end)) & \n",
    "                         (dataset_df.index < pd.to_datetime(test_end))]\n",
    "    print(\"Train dataset: {}, Val dataset: {}, Test dataset: {}\".format(train_df.shape[0], val_df.shape[0], test_df.shape[0]))\n",
    "    \n",
    "    # Split into feats and labels\n",
    "    train_X, train_y = train_df[feats].values, train_df[label_col]\n",
    "    val_X, val_y = val_df[feats].values, val_df[label_col]\n",
    "    test_X, test_y = test_df[feats].values, test_df[label_col]\n",
    "\n",
    "    # Reshape X-inputs to be 3D for LSTM [samples, timesteps, features]\n",
    "    train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "    val_X = val_X.reshape((val_X.shape[0], 1, val_X.shape[1]))\n",
    "    test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "\n",
    "    return train_X, train_y, val_X, val_y, test_X, test_y, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab688a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Methods\n",
    "\n",
    "def _attention_base_model(n_feats, dropout, attn_key_dim, layer_dense_1_units, layer_dense_2_units, \n",
    "                          layer_dense_3_units):\n",
    "    input_layer = tf.keras.layers.Input(shape=(1, n_feats))\n",
    "    x = tf.keras.layers.MultiHeadAttention(num_heads=8, key_dim=attn_key_dim)(query=input_layer, \n",
    "                                                                              value=input_layer, \n",
    "                                                                              key=input_layer)\n",
    "    x = tf.keras.layers.Dense(units=layer_dense_1_units)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(units=layer_dense_2_units)(x)\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "    x = tf.keras.layers.Dense(units=layer_dense_3_units)(x)\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "    output_layer = tf.keras.layers.Dense(units=1)(x)\n",
    "    model = tf.keras.models.Model(inputs=input_layer, outputs=output_layer) \n",
    "    return model\n",
    "\n",
    "# Losses Methods\n",
    "\n",
    "def _compile_mse_model(model):\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min', restore_best_weights=True)\n",
    "    model.summary()\n",
    "    return model, early_stop\n",
    "\n",
    "def _compile_adamw_mse_model(model, learning_rate, weight_decay):\n",
    "    # AdamW \n",
    "    optimizer = AdamW(learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min', restore_best_weights=True)\n",
    "    model.summary()\n",
    "    return model, early_stop\n",
    "\n",
    "def _compile_adadelta_mse_model(model, learning_rate, rho):\n",
    "    # Adadelta is based on adaptive learning rate\n",
    "    optimizer = Adadelta(learning_rate=learning_rate, rho=rho)\n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min', restore_best_weights=True)\n",
    "    model.summary()\n",
    "    return model, early_stop\n",
    "\n",
    "# Training Methods\n",
    "\n",
    "def _fit_model(model, epochs, train_X, train_y, val_X, val_y, callbacks):    \n",
    "    history = model.fit(train_X, train_y, \n",
    "                        epochs=epochs,\n",
    "                        validation_data=(val_X, val_y), \n",
    "                        verbose=1, \n",
    "                        callbacks=callbacks,\n",
    "                        shuffle=True)\n",
    "    return history\n",
    "    \n",
    "def _run_experiment(comet_api_key, comet_workspace, comet_project, data_df, clusters_df,\n",
    "                    cluster_id, tag,\n",
    "                    model_name, batch_size, loss, learning_rate, weight_decay, rho, epochs,\n",
    "                    dropout, attn_key_dim, layer_dense_1_units, layer_dense_2_units, layer_dense_3_units,\n",
    "                    original_feats, meteo_feats, pollen_feats, date_col, label_col,\n",
    "                    train_end, val_end, test_end):\n",
    "\n",
    "    model_id = \"{}-Batch{}-Loss{}-Cluster{}\".format(model_name, batch_size, loss, cluster_id)\n",
    "    experiment = _create_experiment(comet_api_key, comet_workspace, comet_project)\n",
    "    experiment.set_name(model_id)\n",
    "    experiment.add_tag(tag)\n",
    "    print(\"Running Experiment {}:\".format(model_id))\n",
    "\n",
    "    # Get Data\n",
    "    print(\"\\nGetting data..\")\n",
    "    dataset_df = _get_data(data_df, clusters_df, cluster_id, original_feats, date_col, label_col)    \n",
    "\n",
    "    # Prepare Data\n",
    "    print(\"\\nPreparing data..\")\n",
    "    dataset_df, feats, n_feats, cols_max = _prepare_data(dataset_df, original_feats, meteo_feats, \n",
    "                                                         pollen_feats, label_col)\n",
    "\n",
    "    # Create Dataset\n",
    "    print(\"\\nCreating dataset..\")\n",
    "    train_X, train_y, val_X, val_y, test_X, test_y, test_df = _create_datasets(dataset_df, train_end, val_end,\n",
    "                                                                               test_end, feats, label_col, \n",
    "                                                                               batch_size)\n",
    "\n",
    "    # Define Model\n",
    "    print(\"\\nDefining & Training model..\")\n",
    "    if model_name == 'Attention_base':\n",
    "        model = _attention_base_model(n_feats, dropout, attn_key_dim, layer_dense_1_units, layer_dense_2_units, \n",
    "                                      layer_dense_3_units)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # Compile Model\n",
    "    if loss == 'ADAM_MSE':\n",
    "        model, early_stop = _compile_mse_model(model)\n",
    "    elif loss == 'ADAMW_MSE':\n",
    "        model, early_stop = _compile_adamw_mse_model(model, learning_rate, weight_decay)\n",
    "    elif loss == 'ADADELTA_MSE':\n",
    "        model, early_stop = _compile_adadelta_mse_model(model, learning_rate, rho)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # Train Model\n",
    "    history = _fit_model(model, epochs, train_X, train_y, val_X, val_y, [early_stop])\n",
    "\n",
    "    # Get Error on test-set\n",
    "    preds, error = _get_error(model, test_X, test_y)\n",
    "    experiment.log_other(\"test-error\", error)\n",
    "    print(\"Final Error: {}\".format(error))\n",
    "    \n",
    "    # Save Model (locally and on comet)\n",
    "    model_path = os.path.join(MODEL_DIR, model_id)\n",
    "    print(\"Saving Model on {} ..\".format(model_path))\n",
    "    model.save(model_path)\n",
    "    experiment.log_model(model_id, model_path)\n",
    "\n",
    "    # End experiment\n",
    "    experiment.end()\n",
    "    \n",
    "    return history, test_X, test_y, test_df, preds, error, feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40bde03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Methods\n",
    "\n",
    "def _get_error(model, test_X, test_y):\n",
    "    preds = model.predict(test_X).squeeze()    \n",
    "    error = np.mean(np.abs(preds-test_y))\n",
    "    return preds, error\n",
    "\n",
    "def _feats_importances(test_X, test_y, feats):\n",
    "    # Get baseline error\n",
    "    feats_imp = []\n",
    "    ff_preds = model.predict(test_X, verbose=0).squeeze()\n",
    "    ff_x, ff_y = test_X, test_y\n",
    "    ff_x, ff_y = np.array(ff_x), np.array(ff_y)\n",
    "    baseline_error = np.mean(np.abs(ff_preds-np.array(ff_y)))\n",
    "    feats_imp.append({'feature':'BASELINE','mae': baseline_error})\n",
    "    \n",
    "    # Get features gain on reducing error: each value \n",
    "    for k in tqdm(range(len(feats))):\n",
    "        # Change values for current feat\n",
    "        save_col = ff_x[:,:,k].copy()\n",
    "        ff_x[:,:,k] = -100\n",
    "        # Compute error \n",
    "        oof_preds = model.predict(ff_x, verbose=0).squeeze() \n",
    "        mae = np.mean(np.abs(oof_preds-ff_y))\n",
    "        feats_imp.append({'feature': feats[k],'mae': mae})\n",
    "        ff_x[:,:,k] = save_col\n",
    "    \n",
    "    # Plot \n",
    "    df = pd.DataFrame(feats_imp)\n",
    "    df = df.sort_values('mae')\n",
    "    plt.figure(figsize=(10, 13))\n",
    "    plt.barh(np.arange(len(feats)+1), df.mae)\n",
    "    plt.yticks(np.arange(len(feats)+1), df.feature.values)\n",
    "    plt.title('LSTM Feature Importance', size=16)\n",
    "    plt.ylim((-1, len(feats)+1))\n",
    "    plt.plot([baseline_error, baseline_error], [-1,len(feats)+1], '--', color='orange',\n",
    "             label=f'Baseline OOF\\nMAE={baseline_error:.3f}')\n",
    "    plt.xlabel('MAE with feature permuted', size=14)\n",
    "    plt.ylabel('Feature', size=14)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def _plot_history(history):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def _plot_preds(preds, test_df, label_col):\n",
    "    # Preds\n",
    "    for i in range(len(preds), test_df.shape[0]):\n",
    "        preds = np.append(preds, 0.0)\n",
    "    test_df['preds'] = preds\n",
    "    # Plot some station & bcode\n",
    "    N_SAMPLE = 10\n",
    "    for station_id, pol_var_id in test_df[['station_id', 'pol_var_id']].drop_duplicates().sample(N_SAMPLE).values:\n",
    "        curr_test_df = test_df[(test_df['station_id']==station_id) & (test_df['pol_var_id']==pol_var_id)]\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.title(\"{} - {}\".format(station_id, pol_var_id))\n",
    "        plt.plot(curr_test_df.index, curr_test_df['preds'], label='pred')\n",
    "        plt.plot(curr_test_df.index, curr_test_df[label_col], label='truth')\n",
    "        plt.legend()\n",
    "        plt.ylim(0, 1)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02294648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3b476b0",
   "metadata": {},
   "source": [
    "<h3>1. Config</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8487293b",
   "metadata": {},
   "source": [
    "<h4>1.1 Config BigQuery</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d526cc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Client\n",
    "\n",
    "bq_client = bigquery.Client(project=PROJECT_ID)\n",
    "bq_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5ae261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e6b9aa3",
   "metadata": {},
   "source": [
    "<h3>2. Read Data</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185c05b0",
   "metadata": {},
   "source": [
    "<h4>2.1 Read Cluster file</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fac3602",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_df = pd.read_csv(OUTPUT_CLUSTER_FILENAME)\n",
    "print(clusters_df.shape)\n",
    "clusters_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42672b51",
   "metadata": {},
   "source": [
    "<h4>2.2 Read Optimal Params (if setted)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91f53b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(OPT_PARAMS_FILENAME):\n",
    "    print(\"Params File Not Found.\")    \n",
    "\n",
    "params_df = pd.read_csv(OPT_PARAMS_FILENAME)\n",
    "print(params_df.shape)\n",
    "params_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b74ed53",
   "metadata": {},
   "source": [
    "<h4>2.2 Read Tables</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d44bb4",
   "metadata": {},
   "source": [
    "<b>SLIDING_WINDOWS_DATASET</b> joins meteo features of last 7-days with the next-day pollen value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6e59922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read SLIDING_WINDOWS_DATASET\n",
    "\n",
    "sliding_windows_dataset_df = _read_table_delta(bq_client, PROJECT_ID, JOINED_BQ_DATASET, \n",
    "                                               \"SLIDING_WINDOWS_DATASET\", \"date\",\n",
    "                                               COMMON_PERIOD_INIT, COMMON_PERIOD_END)\n",
    "sliding_windows_dataset_df['date'] = sliding_windows_dataset_df['date'].astype(\"str\")\n",
    "print(sliding_windows_dataset_df.shape)\n",
    "sliding_windows_dataset_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c87666c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6270912c",
   "metadata": {},
   "source": [
    "<h3>3. Run Experiment</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba676c3",
   "metadata": {},
   "source": [
    "<h4>3.1 Config Experiment</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0ac024c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data\n",
    "\n",
    "data_df = sliding_windows_dataset_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6c9d447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Clusters\n",
    "\n",
    "clusters = sorted(clusters_df.cluster.unique())\n",
    "print(\"Found {} clusters in data\".format(len(clusters)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08524cfb",
   "metadata": {},
   "source": [
    "<h4>3.2 Set Params</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578fb1e8",
   "metadata": {},
   "source": [
    "Set:\n",
    "- <b>Manual Params</b>: to use the params setted in the code\n",
    "- <b>Opt Params</b>: to read the optimal params from the output file after hyper-params tuning has run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf8c012c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Params\n",
    "\n",
    "exp_wdgt_value = _create_experiment_widget()\n",
    "exp_wdgt_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e13092",
   "metadata": {},
   "source": [
    "<h4>3.3 Run Experiment</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbec03a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default params\n",
    "\n",
    "DEFAULT_CLUSTER_ID = 4\n",
    "DEFAULT_BATCH_SIZE = 512              \n",
    "DEFAULT_LEARNING_RATE = 0.001\n",
    "DEFAULT_WEIGHT_DECAY = 0.004\n",
    "DEFAULT_RHO = 0.95\n",
    "DEFAULT_EPOCHS = 50\n",
    "DEFAULT_MODEL_NAME = 'Attention_base'  # Attention_base\n",
    "DEFAULT_LOSS = 'ADAM_MSE'              # ADAM_MSE, ADAMW_MSE, ADADELTA_MSE\n",
    "\n",
    "DEFAULT_DROPOUT = 0.4\n",
    "DEFAULT_ATTN_KEY_DIM = 64\n",
    "DEFAULT_LAYER_DENSE_1_UNITS = 64\n",
    "DEFAULT_LAYER_DENSE_2_UNITS = 128\n",
    "DEFAULT_LAYER_DENSE_3_UNITS = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbdf53c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "if exp_wdgt_value.value == 'Manual Params':\n",
    "    # Set params\n",
    "    TAG = \"\"\n",
    "    sample_cluster_id = DEFAULT_CLUSTER_ID\n",
    "    model_name = DEFAULT_MODEL_NAME\n",
    "    batch_size = DEFAULT_BATCH_SIZE\n",
    "    learning_rate = DEFAULT_LEARNING_RATE\n",
    "    weight_decay = DEFAULT_WEIGHT_DECAY\n",
    "    rho = DEFAULT_RHO    \n",
    "    loss = DEFAULT_LOSS\n",
    "    epochs = DEFAULT_EPOCHS\n",
    "    dropout = DEFAULT_DROPOUT\n",
    "    attn_key_dim = DEFAULT_ATTN_KEY_DIM\n",
    "    layer_dense_1_units = DEFAULT_LAYER_DENSE_1_UNITS\n",
    "    layer_dense_2_units = DEFAULT_LAYER_DENSE_2_UNITS\n",
    "    layer_dense_3_units = DEFAULT_LAYER_DENSE_3_UNITS    \n",
    "    # Run experiment\n",
    "    history, test_X, test_y, test_df, preds, error, feats = _run_experiment(COMET_API_KEY, COMET_WORKSPACE, \n",
    "                                                                          COMET_PROJECT_NAME, data_df, \n",
    "                                                                          clusters_df, sample_cluster_id, TAG, \n",
    "                                                                          model_name, batch_size, \n",
    "                                                                          loss, learning_rate, weight_decay, rho,\n",
    "                                                                          epochs, \n",
    "                                                                          dropout, attn_key_dim, \n",
    "                                                                          layer_dense_1_units, \n",
    "                                                                          layer_dense_2_units, \n",
    "                                                                          layer_dense_3_units,\n",
    "                                                                          ORIGINAL_FEATS, \n",
    "                                                                          METEO_FEATS, POLLEN_FEATS, DATE_COL, \n",
    "                                                                          LABEL_COL, TRAIN_END, VAL_END, TEST_END)\n",
    "    # Save results\n",
    "    results[sample_cluster_id] = {\n",
    "        'history': history,\n",
    "        'test_X': test_X, \n",
    "        'test_y': test_y,\n",
    "        'test_df': test_df, \n",
    "        'preds': preds, \n",
    "        'error': error,\n",
    "        'feats': feats\n",
    "    }\n",
    "    print(\"Done.\")\n",
    "    \n",
    "elif exp_wdgt_value.value == 'Opt Params':\n",
    "    # For each cluster\n",
    "    for cluster_id in clusters:\n",
    "        print(\"Running Cluster_id: {}\".format(cluster_id))\n",
    "        TAG = \"OPT\"        \n",
    "        # Get Cluster Opt params\n",
    "        params = params_df[params_df['cluster_id']==cluster_id]\n",
    "        model_name = params['model_name'].values[0]\n",
    "        batch_size = params['batch_size'].values[0]\n",
    "        learning_rate = params['learning_rate'].values[0]\n",
    "        weight_decay = DEFAULT_WEIGHT_DECAY\n",
    "        rho = DEFAULT_RHO\n",
    "        loss = DEFAULT_LOSS\n",
    "        epochs = DEFAULT_EPOCHS            \n",
    "        dropout = params['dropout'].values[0]\n",
    "        attn_key_dim = params['attention_key_dim'].values[0]\n",
    "        layer_dense_1_units = params['layer_dense_1_units'].values[0]\n",
    "        layer_dense_2_units = params['layer_dense_2_units'].values[0]\n",
    "        layer_dense_3_units = params['layer_dense_3_units'].values[0]        \n",
    "        # Run experiment\n",
    "        history, test_X, test_y, test_df, preds, error, feats = _run_experiment(COMET_API_KEY, COMET_WORKSPACE, \n",
    "                                                                              COMET_PROJECT_NAME, data_df, \n",
    "                                                                              clusters_df, cluster_id, TAG,\n",
    "                                                                              model_name, batch_size, \n",
    "                                                                              loss, learning_rate, \n",
    "                                                                              weight_decay, rho, epochs, \n",
    "                                                                              dropout, attn_key_dim, \n",
    "                                                                              layer_dense_1_units, \n",
    "                                                                              layer_dense_2_units, \n",
    "                                                                              layer_dense_3_units,\n",
    "                                                                              ORIGINAL_FEATS, METEO_FEATS, \n",
    "                                                                              POLLEN_FEATS, DATE_COL, LABEL_COL, \n",
    "                                                                              TRAIN_END, VAL_END, TEST_END)\n",
    "        # Save results\n",
    "        results[cluster_id] = {\n",
    "            'history': history,\n",
    "            'test_X': test_X, \n",
    "            'test_y': test_y,\n",
    "            'test_df': test_df, \n",
    "            'preds': preds, \n",
    "            'error': error,\n",
    "            'feats': feats\n",
    "        }\n",
    "        print(\"Done.\")\n",
    "    \n",
    "else:\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932e5214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72e37787",
   "metadata": {},
   "source": [
    "<h3>4. Evaluate Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19f04bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot History & Preds for each cluster\n",
    "\n",
    "for cluster_id, result in results.items():\n",
    "    print(\"Cluster_id: {}\".format(cluster_id))\n",
    "    print(\"MSE Error: {}\".format(result['error']))\n",
    "    # Plot History\n",
    "    _plot_history(result['history'])\n",
    "\n",
    "    # Plot Feat Importances\n",
    "    params = params_df[params_df['cluster_id']==cluster_id]\n",
    "    _feats_importances(result['test_X'], result['test_y'], result['feats'])\n",
    "\n",
    "    # Plot Preds\n",
    "    _plot_preds(preds, result['test_df'], LABEL_COL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd6b0d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3620990a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98bdd1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fa2a31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
