{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27e9f867",
   "metadata": {},
   "source": [
    "# Fine-Tuning on Cluster TSD Season\n",
    "Notebook to perform fine-tuning on the best architectur for the TSD Season Cluster: trhougth time-series-decomposition, the season inside the cluster is extracted and used to make prediction.<br>\n",
    "Data read are from table SLIDING_WINDOWS_DATASET that contains a sliding windows of:\n",
    "- feats about last 7-days meteo values\n",
    "- pollen value for the next day\n",
    "\n",
    "Cluster associations are read from a local file: we have different cluster annotations made by different techniques.<br>\n",
    "We explore different model & hyper-parameters throught Comet ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96a293a",
   "metadata": {},
   "source": [
    "<h3>Import</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80880ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 13:03:39.636243: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.18.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from google.cloud import bigquery\n",
    "\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import get_cmap\n",
    "from matplotlib import cm\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "import ipywidgets as widgets\n",
    "import scipy.stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "\n",
    "from comet_ml import Experiment, Optimizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers.experimental import Adam, AdamW, Adadelta\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "my_cmap = plt.get_cmap(\"Paired\")\n",
    "init_notebook_mode(connected=True)  \n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7c4047",
   "metadata": {},
   "source": [
    "<h3>Config</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7031950c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "\n",
    "PROJECT_ID = 'arpae-prod-ml'\n",
    "\n",
    "# BigQuery\n",
    "BQ_DATASET = 'SAMPLE_DATA'\n",
    "JOINED_BQ_DATASET = 'JOINED_DATA'\n",
    "\n",
    "# Const\n",
    "COMMON_PERIOD_INIT = '2011-01-01'\n",
    "COMMON_PERIOD_END = '2021-12-31' \n",
    "\n",
    "TRAIN_END = '2016-12-31 00:00:00+00:00'\n",
    "VAL_END = '2019-12-31 00:00:00+00:00'\n",
    "TEST_END = '2022-12-31 00:00:00+00:00'\n",
    "\n",
    "# Cols\n",
    "DATE_COL = 'date'\n",
    "\n",
    "# Feats\n",
    "METEO_FEATS = ['week_amax', \n",
    "               'station_lat_amax', 'station_lon_amax', 'station_H_piano_strada_amax', 'station_H_mslm_amax', \n",
    "               'B13011_min_amin', 'B13011_max_amax', 'B13011_mean_mean', 'B13011_std_mean', 'B13011_sum_sum', \n",
    "               'B14198_min_amin', 'B14198_max_amax', 'B14198_mean_mean', 'B14198_std_mean', 'B14198_sum_sum',\n",
    "               'TEMP_min_amin', 'TEMP_max_amax', 'TEMP_mean_mean', 'TEMP_std_mean', 'TEMP_sum_sum',                                               \n",
    "               'PREC_amin', 'PREC_mean', 'PREC_std', 'PREC_median', 'PREC_amax', 'PREC_skew', 'PREC_kurtosis']\n",
    "POLLEN_FEATS = ['seasonal_mean', 'seasonal_prev_1', # seasonal, trend, residual\n",
    "                'pol_value_amin', 'pol_value_mean', 'pol_value_std', 'pol_value_median', 'pol_value_amax', \n",
    "                'pol_value_skew', 'pol_value_kurtosis',\n",
    "                'pol_value_prev_1', 'pol_value_prev_2', 'pol_value_prev_3',\n",
    "                'pol_value_prev_4', 'pol_value_prev_5', 'pol_value_prev_6',\n",
    "                'pol_value_prev_7']\n",
    "ORIGINAL_FEATS = METEO_FEATS + POLLEN_FEATS\n",
    "LABEL_COL = 'season_label' # season, trend, residual\n",
    "\n",
    "# Params\n",
    "EPOCHS = 50\n",
    "\n",
    "# Comet Params\n",
    "COMET_API_KEY = 'B4Tttbbx4JrwXD9x2HBNjCdXX'\n",
    "COMET_WORKSPACE = 'pveronesi' \n",
    "COMET_PROJECT_NAME = 'arpae-tsd-season-finetuning' # season, trend, residual\n",
    "\n",
    "# Layout\n",
    "COLOR_PALETTE = px.colors.qualitative.Prism\n",
    "\n",
    "OUTPUT_CLUSTER_FILENAME = \"../../data/clustering_season_intervals.csv\" # season, trend, residual\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b40c5c9",
   "metadata": {},
   "source": [
    "<h3>Methods</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccb72ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Methods\n",
    "\n",
    "def _run_query(client, query): \n",
    "    df = client.query(query).to_dataframe()\n",
    "    return df\n",
    "\n",
    "def _read_table(client, project_id, dataset, table):\n",
    "    query = \"SELECT * FROM `{}.{}.{}` \".format(project_id, dataset, table)\n",
    "    df = _run_query(client, query)\n",
    "    return df\n",
    "\n",
    "def _read_table_delta(client, project_id, dataset, table, date_col, init, end):\n",
    "    query = \"SELECT * FROM `{}.{}.{}` WHERE {} > '{}' AND {} < '{}' \".format(project_id, dataset, table, date_col, init, date_col, end)\n",
    "    df = _run_query(client, query)\n",
    "    if 'reftime' in df.columns:\n",
    "        df.sort_values(by='reftime', inplace=True)\n",
    "    elif date_col in df.columns:\n",
    "        df.sort_values(by=date_col, inplace=True)\n",
    "    else:\n",
    "        return None\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cf45c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comet methods\n",
    "\n",
    "def _create_experiment(api_key, workspace, project_name):\n",
    "    experiment = Experiment(\n",
    "        # user config\n",
    "        api_key=api_key,\n",
    "        workspace=workspace,  \n",
    "        # project config\n",
    "        project_name=project_name,\n",
    "        # logging config\n",
    "        log_code=True,\n",
    "        log_graph=True,\n",
    "        auto_param_logging=True,\n",
    "        auto_metric_logging=True,    \n",
    "        auto_histogram_weight_logging=True,\n",
    "        auto_histogram_gradient_logging=True,\n",
    "        auto_histogram_activation_logging=True\n",
    "    )\n",
    "    return experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36411aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Methods\n",
    "\n",
    "def _create_cluster_widget(clusters):\n",
    "    cluster_wdgt = widgets.Dropdown(options=clusters, description='Cluster id:', layout={\"width\":\"50%\"})\n",
    "    return cluster_wdgt\n",
    "\n",
    "def _normalize(x, range_dict, index_col, label_col):\n",
    "    pol_min = range_dict[x[index_col]]['min']\n",
    "    pol_max = range_dict[x[index_col]]['max']\n",
    "    return (x[label_col] - pol_min) / (pol_max - pol_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37550651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Methods\n",
    "\n",
    "def _get_data(data_df, clusters_df, cluster_id, feats_cols, date_col, label_col):\n",
    "    # filter data\n",
    "    filt_clusters_df = clusters_df[clusters_df['cluster']==cluster_id][['station_id', 'pol_var_id']]\n",
    "    dataset_df = pd.merge(data_df, filt_clusters_df, how='right', on=['station_id', 'pol_var_id'])\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset_df.sort_values(['station_id', 'pol_var_id', date_col], inplace=True)\n",
    "    dataset_df = dataset_df[['station_id', 'pol_var_id', date_col]  + feats_cols + [label_col]]\n",
    "    \n",
    "    # Set index and drop nan\n",
    "    dataset_df.set_index(date_col, inplace=True)\n",
    "    dataset_df.dropna(inplace=True)\n",
    "    \n",
    "    print(\"Rows: {}\".format(dataset_df.shape[0]))\n",
    "    return dataset_df\n",
    "\n",
    "def _prepare_data(dataset_df, original_feats, meteo_feats, pollen_feats, label_col):\n",
    "    # Add 1-hot encoding cols\n",
    "    stations_one_hot = pd.get_dummies(dataset_df['station_id'], prefix='station_id')\n",
    "    pollen_one_hot = pd.get_dummies(dataset_df['pol_var_id'], prefix='pol_var_id')\n",
    "    dataset_df = pd.concat([dataset_df, stations_one_hot], axis=1)\n",
    "    dataset_df = pd.concat([dataset_df, pollen_one_hot], axis=1)\n",
    "    \n",
    "    # Update Features\n",
    "    feats = original_feats + stations_one_hot.columns.values.tolist() + pollen_one_hot.columns.values.tolist()\n",
    "    n_feats = len(feats)\n",
    "    \n",
    "    # Normalize all cols except for pollen ones; Save max cols to restore original values\n",
    "    cols_max = []\n",
    "    for col in meteo_feats:\n",
    "        scaler = MinMaxScaler()\n",
    "        dataset_df[col] = pd.DataFrame(scaler.fit_transform(dataset_df[[col]])).values\n",
    "        cols_max.append(int(scaler.data_max_))\n",
    "\n",
    "    # Normalize with Min-Max scaling each pollen feats \n",
    "    for col in tqdm(pollen_feats):\n",
    "        range_df = dataset_df[['pol_var_id', col]].groupby('pol_var_id').agg(['min', 'max'])\n",
    "        ranges = {}\n",
    "        for index, values in zip(range_df.index, range_df.values):\n",
    "            ranges[index] = {'min': values[0], 'max': values[1]}        \n",
    "        dataset_df[col] = dataset_df.apply(lambda x: _normalize(x, ranges, 'pol_var_id', col), axis=1)    \n",
    "    \n",
    "    # Normalize with Min-Max scaling the label col\n",
    "    range_df = dataset_df[['pol_var_id', LABEL_COL]].groupby('pol_var_id').agg(['min', 'max'])\n",
    "    ranges = {}\n",
    "    for index, values in zip(range_df.index, range_df.values):\n",
    "        ranges[index] = {'min': values[0], 'max': values[1]}        \n",
    "    dataset_df[label_col] = dataset_df.apply(lambda x: _normalize(x, ranges, 'pol_var_id', label_col), axis=1)    \n",
    "    \n",
    "    # Sort data\n",
    "    dataset_df.index = pd.to_datetime(dataset_df.index)\n",
    "    dataset_df.sort_values(by=['station_id', 'pol_var_id', 'date'], inplace=True)\n",
    "\n",
    "    return dataset_df, feats, n_feats, cols_max\n",
    "\n",
    "def _create_datasets(experiment, dataset_df, train_end, val_end, test_end, feats, label_col):\n",
    "    # Get params\n",
    "    sequence_len = experiment.get_parameter(\"sequence_len\")\n",
    "    batch_size = experiment.get_parameter(\"batch_size\")\n",
    "    \n",
    "    # Split df into train and test sets\n",
    "    train_df = dataset_df[dataset_df.index < pd.to_datetime(train_end)]\n",
    "    val_df = dataset_df[(dataset_df.index > pd.to_datetime(train_end)) & \n",
    "                        (dataset_df.index < pd.to_datetime(val_end))]\n",
    "    test_df = dataset_df[(dataset_df.index > pd.to_datetime(val_end)) & \n",
    "                         (dataset_df.index < pd.to_datetime(test_end))]\n",
    "    print(\"Train dataset: {}, Val dataset: {}, Test dataset: {}\".format(train_df.shape[0], \n",
    "                                                                        val_df.shape[0], \n",
    "                                                                        test_df.shape[0]))\n",
    "    \n",
    "    # Split into feats and labels\n",
    "    train_X, train_y = train_df[feats].values, train_df[label_col]\n",
    "    val_X, val_y = val_df[feats].values, val_df[label_col]\n",
    "    test_X, test_y = test_df[feats].values, test_df[label_col]\n",
    "\n",
    "    # Create Sliding-Windows Dataset\n",
    "    train_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(train_X,\n",
    "                                                                         train_y,\n",
    "                                                                         sequence_length=sequence_len,\n",
    "                                                                         batch_size=batch_size)\n",
    "    val_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(val_X,\n",
    "                                                                       val_y,\n",
    "                                                                       sequence_length=sequence_len,\n",
    "                                                                       batch_size=batch_size)\n",
    "    test_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(test_X,\n",
    "                                                                        test_y,\n",
    "                                                                        sequence_length=sequence_len,\n",
    "                                                                        batch_size=batch_size)\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bbf78a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Methods\n",
    "\n",
    "def _lstm_base_model(experiment, n_feats):\n",
    "    # Get params\n",
    "    sequence_len = experiment.get_parameter(\"sequence_len\")\n",
    "    layer_lstm_1_units = experiment.get_parameter(\"layer_lstm_1_units\")\n",
    "    layer_lstm_2_units = experiment.get_parameter(\"layer_lstm_2_units\")\n",
    "    layer_dense_units = experiment.get_parameter(\"layer_dense_units\")\n",
    "    dropout = experiment.get_parameter(\"dropout\")\n",
    "    \n",
    "    # Create Model\n",
    "    input_layer = tf.keras.layers.Input(shape=(sequence_len, n_feats))\n",
    "    x = tf.keras.layers.LSTM(units=layer_lstm_1_units, \n",
    "                                  dropout=dropout, \n",
    "                                  return_sequences=True)(input_layer)\n",
    "    x = tf.keras.layers.LSTM(units=layer_lstm_2_units, \n",
    "                                  dropout=dropout, \n",
    "                                  return_sequences=True)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dropout(0.4)(x)\n",
    "    x = tf.keras.layers.Dense(units=layer_dense_units)(x)\n",
    "    x = tf.keras.layers.Dropout(0.4)(x)\n",
    "    output_layer = tf.keras.layers.Dense(units=1)(x)\n",
    "    model = tf.keras.models.Model(inputs=input_layer, outputs=output_layer) \n",
    "    return model\n",
    "\n",
    "# Losses Methods\n",
    "\n",
    "def _compile_mse_model(experiment, model):\n",
    "    # Get params\n",
    "    learning_rate = experiment.get_parameter(\"learning_rate\")\n",
    "    # Compile\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min', restore_best_weights=True)\n",
    "    model.summary()\n",
    "    return model, early_stop\n",
    "\n",
    "# Training Methods\n",
    "\n",
    "def _fit_model(model, epochs, train_dataset, val_dataset, callbacks):    \n",
    "    history = model.fit(train_dataset, \n",
    "                        epochs=epochs,\n",
    "                        validation_data=val_dataset, \n",
    "                        verbose=1, \n",
    "                        callbacks=[early_stop],\n",
    "                        shuffle=True)\n",
    "    return history\n",
    "    \n",
    "# Evaluation Methods\n",
    "\n",
    "def _get_error(test_dataset):\n",
    "    preds = model.predict(test_dataset).squeeze()\n",
    "    truth = []\n",
    "    for x, y in test_dataset:\n",
    "        truth.extend(y.numpy())\n",
    "    error = np.mean(np.abs(preds-np.array(truth)))\n",
    "    return preds, error\n",
    "\n",
    "def _feats_importances(test_dataset, feats, sequence_len):\n",
    "    # Get baseline error\n",
    "    feats_imp = []\n",
    "    ff_preds = model.predict(test_dataset, verbose=0).squeeze()\n",
    "    ff_x, ff_y = [], []\n",
    "    for x, y in test_dataset:\n",
    "        ff_x.extend(x.numpy())\n",
    "        ff_y.extend(y.numpy())\n",
    "    ff_x, ff_y = np.array(ff_x), np.array(ff_y)\n",
    "    baseline_error = np.mean(np.abs(ff_preds-np.array(ff_y)))\n",
    "    feats_imp.append({'feature':'BASELINE','mae': baseline_error})\n",
    "    \n",
    "    # Get features gain on reducing error: each value \n",
    "    for k in tqdm(range(len(feats))):\n",
    "        # Change values for current feat\n",
    "        save_col = ff_x[:,:,k].copy()\n",
    "        ff_x[:,:,k] = [-100 for x in range(sequence_len)]\n",
    "        # Compute error \n",
    "        oof_preds = model.predict(ff_x, verbose=0).squeeze() \n",
    "        mae = np.mean(np.abs(oof_preds-ff_y))\n",
    "        feats_imp.append({'feature': feats[k],'mae': mae})\n",
    "        ff_x[:,:,k] = save_col\n",
    "    \n",
    "    # Plot \n",
    "    df = pd.DataFrame(feats_imp)\n",
    "    df = df.sort_values('mae')\n",
    "    plt.figure(figsize=(10, 13))\n",
    "    plt.barh(np.arange(len(feats)+1), df.mae)\n",
    "    plt.yticks(np.arange(len(feats)+1), df.feature.values)\n",
    "    plt.title('LSTM Feature Importance', size=16)\n",
    "    plt.ylim((-1, len(feats)+1))\n",
    "    plt.plot([baseline_error, baseline_error], [-1,len(feats)+1], '--', color='orange',\n",
    "             label=f'Baseline OOF\\nMAE={baseline_error:.3f}')\n",
    "    plt.xlabel('MAE with feature permuted', size=14)\n",
    "    plt.ylabel('Feature', size=14)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def _predict(preds, test_df, label_col):\n",
    "    try:\n",
    "        # Predict\n",
    "        for i in range(len(preds), test_df.shape[0]):\n",
    "            preds = np.append(preds, 0.0)\n",
    "        test_df['preds'] = preds\n",
    "        # Plot some station & bcode\n",
    "        n_sample = min(10, test_df[['station_id', 'pol_var_id']].drop_duplicates().shape[0])\n",
    "        for station_id, pol_var_id in test_df[['station_id', 'pol_var_id']].drop_duplicates().sample(n_sample).values:\n",
    "            curr_test_df = test_df[(test_df['station_id']==station_id) & (test_df['pol_var_id']==pol_var_id)]\n",
    "            curr_test_df.sort_index(inplace=True)\n",
    "            plt.figure(figsize=(15, 5))\n",
    "            plt.title(\"{} - {}\".format(station_id, pol_var_id))\n",
    "            plt.plot(curr_test_df.index, curr_test_df['preds'], label='pred')\n",
    "            plt.plot(curr_test_df.index, curr_test_df[label_col], label='truth')\n",
    "            plt.ylim(0, 1)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeeb4fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3b476b0",
   "metadata": {},
   "source": [
    "<h3>1. Config</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8487293b",
   "metadata": {},
   "source": [
    "<h4>1.1 Config BigQuery</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d526cc1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.client.Client at 0x11069a820>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup Client\n",
    "\n",
    "bq_client = bigquery.Client(project=PROJECT_ID)\n",
    "bq_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6b9aa3",
   "metadata": {},
   "source": [
    "<h3>2. Read Data</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185c05b0",
   "metadata": {},
   "source": [
    "<h4>2.1 Read Cluster file</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fac3602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(363, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>station_id</th>\n",
       "      <th>pol_var_id</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>B48001</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>B48002</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>B48003</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  station_id pol_var_id  cluster\n",
       "0           0           1     B48001        4\n",
       "1           1           1     B48002        3\n",
       "2           2           1     B48003        0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters_df = pd.read_csv(OUTPUT_CLUSTER_FILENAME)\n",
    "print(clusters_df.shape)\n",
    "clusters_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b74ed53",
   "metadata": {},
   "source": [
    "<h4>2.2 Read Tables</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d44bb4",
   "metadata": {},
   "source": [
    "<b>SLIDING_WINDOWS_DATASET</b> joins meteo features of last 7-days with the next-day pollen value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6e59922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(783015, 55)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>station_id</th>\n",
       "      <th>pol_var_id</th>\n",
       "      <th>date_diff</th>\n",
       "      <th>B13011_min_amin</th>\n",
       "      <th>B13011_max_amax</th>\n",
       "      <th>B13011_mean_mean</th>\n",
       "      <th>B13011_std_mean</th>\n",
       "      <th>B13011_sum_sum</th>\n",
       "      <th>B14198_min_amin</th>\n",
       "      <th>...</th>\n",
       "      <th>pol_value_prev_2</th>\n",
       "      <th>pol_value_prev_3</th>\n",
       "      <th>pol_value_prev_4</th>\n",
       "      <th>pol_value_prev_5</th>\n",
       "      <th>pol_value_prev_6</th>\n",
       "      <th>pol_value_prev_7</th>\n",
       "      <th>pol_value_label</th>\n",
       "      <th>season_label</th>\n",
       "      <th>trend_label</th>\n",
       "      <th>residual_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>463994</th>\n",
       "      <td>2011-01-02 00:00:00+00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>B48029</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.4</td>\n",
       "      <td>0.015630</td>\n",
       "      <td>0.144917</td>\n",
       "      <td>18.6</td>\n",
       "      <td>-13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.338131</td>\n",
       "      <td>0.829370</td>\n",
       "      <td>0.508761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513242</th>\n",
       "      <td>2011-01-02 00:00:00+00:00</td>\n",
       "      <td>10</td>\n",
       "      <td>B48016</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.8</td>\n",
       "      <td>0.014118</td>\n",
       "      <td>0.146686</td>\n",
       "      <td>16.8</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.770304</td>\n",
       "      <td>3.846548</td>\n",
       "      <td>-1.076244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440463</th>\n",
       "      <td>2011-01-02 00:00:00+00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>B48034</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.014454</td>\n",
       "      <td>0.153349</td>\n",
       "      <td>17.2</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.641494</td>\n",
       "      <td>6.968164</td>\n",
       "      <td>-2.326670</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             date  station_id pol_var_id  date_diff  \\\n",
       "463994  2011-01-02 00:00:00+00:00           8     B48029          9   \n",
       "513242  2011-01-02 00:00:00+00:00          10     B48016          7   \n",
       "440463  2011-01-02 00:00:00+00:00           7     B48034          8   \n",
       "\n",
       "        B13011_min_amin  B13011_max_amax  B13011_mean_mean  B13011_std_mean  \\\n",
       "463994              0.0             11.4          0.015630         0.144917   \n",
       "513242              0.0             12.8          0.014118         0.146686   \n",
       "440463              0.0             13.0          0.014454         0.153349   \n",
       "\n",
       "        B13011_sum_sum  B14198_min_amin  ...  pol_value_prev_2  \\\n",
       "463994            18.6            -13.0  ...               0.0   \n",
       "513242            16.8            -12.0  ...               0.0   \n",
       "440463            17.2            -15.0  ...               0.0   \n",
       "\n",
       "        pol_value_prev_3  pol_value_prev_4  pol_value_prev_5  \\\n",
       "463994               0.0               0.0               0.0   \n",
       "513242               0.0               0.0               0.0   \n",
       "440463               0.0               0.0               0.0   \n",
       "\n",
       "        pol_value_prev_6  pol_value_prev_7  pol_value_label  season_label  \\\n",
       "463994               0.0               0.0              0.0     -1.338131   \n",
       "513242               0.0               0.0              0.0     -2.770304   \n",
       "440463               0.0               0.0              0.0     -4.641494   \n",
       "\n",
       "        trend_label  residual_label  \n",
       "463994     0.829370        0.508761  \n",
       "513242     3.846548       -1.076244  \n",
       "440463     6.968164       -2.326670  \n",
       "\n",
       "[3 rows x 55 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read SLIDING_WINDOWS_DATASET\n",
    "\n",
    "sliding_windows_dataset_df = _read_table_delta(bq_client, PROJECT_ID, JOINED_BQ_DATASET, \n",
    "                                               \"SLIDING_WINDOWS_DATASET\", \"date\",\n",
    "                                               COMMON_PERIOD_INIT, COMMON_PERIOD_END)\n",
    "sliding_windows_dataset_df['date'] = sliding_windows_dataset_df['date'].astype(\"str\")\n",
    "print(sliding_windows_dataset_df.shape)\n",
    "sliding_windows_dataset_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c87666c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6270912c",
   "metadata": {},
   "source": [
    "<h3>3. Run Experiments</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e08b20b",
   "metadata": {},
   "source": [
    "We run a fine-tuning for each cluster.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba676c3",
   "metadata": {},
   "source": [
    "<h4>3.1 Config Experiment</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c0ac024c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data\n",
    "\n",
    "data_df = sliding_windows_dataset_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f6c9d447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 clusters in data: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "# Get Clusters\n",
    "\n",
    "clusters = sorted(clusters_df.cluster.unique())\n",
    "print(\"Found {} clusters in data: {}\".format(len(clusters), clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4846ad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set fixed params\n",
    "\n",
    "MODEL_NAME = 'LSTM_base'  # LSTM_base\n",
    "LOSS = 'ADAM_MSE'                 # ADAM_MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2f1ae1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Hyper-parameters\n",
    "\n",
    "max_combo = 10 \n",
    "hyperparams = {\n",
    "    \"algorithm\": \"bayes\",\n",
    "    \"parameters\": {\n",
    "        \"sequence_len\": {\"type\": \"discrete\", \"values\": [7, 14, 21]},\n",
    "        \"layer_lstm_1_units\": {\"type\": \"discrete\", \"values\": [64, 128, 256, 512]},\n",
    "        \"layer_lstm_2_units\": {\"type\": \"discrete\", \"values\": [64, 128, 256, 512]},\n",
    "        \"layer_dense_units\": {\"type\": \"discrete\", \"values\": [64, 128, 256, 512]},        \n",
    "        \"dropout\": {\"type\": \"float\", \"min\": 0.0, \"max\": 1.0},\n",
    "        \"batch_size\": {\"type\": \"discrete\", \"values\": [256, 512, 1024]},\n",
    "        \"learning_rate\": {\"type\": \"float\", \"min\": 0.0001, \"max\": 0.1},\n",
    "    },\n",
    "    \"spec\": {\"maxCombo\": max_combo, \"objective\": \"minimize\", \"metric\": \"val_loss\"},\n",
    "    \"trials\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b1831c",
   "metadata": {},
   "source": [
    "<h4>3.2 Run Experiment</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e9ac660d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd8d3c3775f443d3ae919855a487c5b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fae958b85308467282968664f2c7dc2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: sklearn. Metrics and hyperparameters can still be logged using Experiment.log_metrics() and Experiment.log_parameters()\n",
      "COMET INFO: Optimizer metrics is 'val_loss' but no logged values found. Experiment ignored in sweep.\n",
      "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: sklearn. Metrics and hyperparameters can still be logged using Experiment.log_metrics() and Experiment.log_parameters()\n",
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.com https://www.comet.com/alessioanghileri-noovle/arpae-tsd-trend-finetuning/fb51be11c22544a4921ed991b6c7f9ea\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Experiment LSTM_Attention_v1-LossADAM_MSE-Cluster0-Exp1:\n",
      "\n",
      "Getting data..\n",
      "Rows: 93488\n",
      "\n",
      "Preparing data..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e8286730e3d48e7956ba995c341d3db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating dataset..\n",
      "Train dataset: 78007, Val dataset: 7906, Test dataset: 7562\n",
      "\n",
      "Defining & Training model..\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 14, 58)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  (None, 14, 128)      95744       ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  (None, 14, 256)      394240      ['lstm_2[0][0]']                 \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 14, 256)     526080      ['lstm_3[0][0]',                 \n",
      " eadAttention)                                                    'lstm_3[0][0]',                 \n",
      "                                                                  'lstm_3[0][0]']                 \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 3584)         0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 3584)         0           ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 64)           229440      ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 64)           0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            65          ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,245,569\n",
      "Trainable params: 1,245,569\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: tensorflow datasets are not currently supported for gradient and activation auto-logging\n",
      "COMET INFO: Ignoring automatic log_parameter('verbose') because 'keras:verbose' is in COMET_LOGGING_PARAMETERS_IGNORE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "305/305 [==============================] - 153s 484ms/step - loss: 174778.5000 - val_loss: 2.4929\n",
      "Epoch 2/50\n",
      "305/305 [==============================] - 158s 517ms/step - loss: 51.3797 - val_loss: 0.0505\n",
      "Epoch 3/50\n",
      "228/305 [=====================>........] - ETA: 47s - loss: 15.1685"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6z/h5gl1rs567sb9kzq90zfly6w0000gv/T/ipykernel_15815/4061517031.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Train Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# Get Error on test-set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/6z/h5gl1rs567sb9kzq90zfly6w0000gv/T/ipykernel_15815/576648871.py\u001b[0m in \u001b[0;36m_fit_model\u001b[0;34m(model, epochs, train_dataset, val_dataset, callbacks)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_fit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     history = model.fit(train_dataset, \n\u001b[0m\u001b[1;32m     44\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/alessioanghileri/Jupyter/Noovle/ARPAE-ML/arpae-ml-analysis/venv/lib/python3.8/site-packages/comet_ml/monkey_patching.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m                     )\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0mreturn_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;31m# Call after callbacks once we have the return value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/alessioanghileri/Jupyter/Noovle/ARPAE-ML/arpae-ml-analysis/venv/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/alessioanghileri/Jupyter/Noovle/ARPAE-ML/arpae-ml-analysis/venv/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1648\u001b[0m                         ):\n\u001b[1;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/alessioanghileri/Jupyter/Noovle/ARPAE-ML/arpae-ml-analysis/venv/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/alessioanghileri/Jupyter/Noovle/ARPAE-ML/arpae-ml-analysis/venv/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/alessioanghileri/Jupyter/Noovle/ARPAE-ML/arpae-ml-analysis/venv/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/alessioanghileri/Jupyter/Noovle/ARPAE-ML/arpae-ml-analysis/venv/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m       (concrete_function,\n\u001b[1;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/alessioanghileri/Jupyter/Noovle/ARPAE-ML/arpae-ml-analysis/venv/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/Desktop/alessioanghileri/Jupyter/Noovle/ARPAE-ML/arpae-ml-analysis/venv/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/alessioanghileri/Jupyter/Noovle/ARPAE-ML/arpae-ml-analysis/venv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run Experiment\n",
    "\n",
    "errors = []\n",
    "for cluster_id in tqdm(clusters):\n",
    "    \n",
    "    # Setup CometML Optimizer\n",
    "    optimizer = Optimizer(\n",
    "        hyperparams,\n",
    "        api_key=COMET_API_KEY, \n",
    "        project_name=COMET_PROJECT_NAME,\n",
    "        workspace=COMET_WORKSPACE,\n",
    "        log_env_details=False,\n",
    "        display_summary_level=0,\n",
    "        verbose=0    \n",
    "    )\n",
    "    \n",
    "    # Loop experiments\n",
    "    for ith, experiment in tqdm(enumerate(optimizer.get_experiments(), start=1), total=max_combo):        \n",
    "        model_id = \"{}-Loss{}-Cluster{}-Exp{}\".format(MODEL_NAME, LOSS, cluster_id, ith)    \n",
    "        experiment.set_name(model_id)\n",
    "        experiment.add_tag(\"cluster_id-{}\".format(cluster_id))\n",
    "        print(\"Running Experiment {}:\".format(model_id))\n",
    "\n",
    "        # Get Data\n",
    "        print(\"\\nGetting data..\")\n",
    "        dataset_df = _get_data(data_df, clusters_df, cluster_id, ORIGINAL_FEATS, DATE_COL, LABEL_COL)    \n",
    "\n",
    "        # Prepare Data\n",
    "        print(\"\\nPreparing data..\")\n",
    "        dataset_df, FEATS, N_FEATS, cols_max = _prepare_data(dataset_df, ORIGINAL_FEATS, METEO_FEATS, \n",
    "                                                             POLLEN_FEATS, LABEL_COL)\n",
    "\n",
    "        # Create Dataset\n",
    "        print(\"\\nCreating dataset..\")\n",
    "        train_dataset, val_dataset, test_dataset, test_df = _create_datasets(experiment, dataset_df, TRAIN_END, \n",
    "                                                                             VAL_END, TEST_END, FEATS, LABEL_COL)\n",
    "\n",
    "        # Define Model\n",
    "        print(\"\\nDefining & Training model..\")\n",
    "        if MODEL_NAME == 'LSTM_base':\n",
    "            model = _lstm_base_model(experiment, N_FEATS)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # Compile Model\n",
    "        if LOSS == 'ADAM_MSE':\n",
    "            model, early_stop = _compile_mse_model(experiment, model)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # Train Model\n",
    "        history = _fit_model(model, EPOCHS, train_dataset, val_dataset, [early_stop])\n",
    "\n",
    "        # Get Error on test-set\n",
    "        preds, error = _get_error(test_dataset)\n",
    "        experiment.log_other(\"{} test-error: {}\".format(cluster_id, error))\n",
    "        print(\"Final Error: {}\".format(error))\n",
    "\n",
    "        # Plot Features importances\n",
    "        _feats_importances(test_dataset, FEATS, sequence_len=experiment.get_parameter(\"sequence_len\"))\n",
    "\n",
    "        # Show Prediction on test-set\n",
    "        _predict(preds, test_df, LABEL_COL)\n",
    "\n",
    "        # End experiment\n",
    "        experiment.end()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932e5214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b6ac55",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c455369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b771a27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3012680",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
